{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    PreTrainedTokenizerFast,\n",
    "    TrainerCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from trl.trainer.sft_trainer import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = int(os.environ.get(\"SEED\", \"42\"))\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "set_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "print({\n",
    "    \"seed\": SEED,\n",
    "    \"device\": str(device),\n",
    "    \"device_name\": device_name,\n",
    "    \"cuda\": torch.cuda.is_available(),\n",
    "    \"dtype\": str(dtype),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_DIR = Path(\"data/corpus\")\n",
    "ALPACA_DIR = Path(\"data/alpaca-cleaned-ru\")\n",
    "\n",
    "assert CORPUS_DIR.exists() and CORPUS_DIR.is_dir(), f\"Missing directory: {CORPUS_DIR}\"\n",
    "assert ALPACA_DIR.exists() and ALPACA_DIR.is_dir(), f\"Missing directory: {ALPACA_DIR}\"\n",
    "\n",
    "corpus_txt_files = sorted(CORPUS_DIR.glob(\"**/*.txt\"))\n",
    "parquet_files = sorted(ALPACA_DIR.glob(\"*.parquet\"))\n",
    "\n",
    "stats = {\n",
    "    \"corpus_num_files\": len(corpus_txt_files),\n",
    "    \"corpus_sample\": [str(p) for p in corpus_txt_files[:3]],\n",
    "    \"alpaca_parquet_files\": [str(p) for p in parquet_files],\n",
    "}\n",
    "\n",
    "print(json.dumps(stats, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(CORPUS_DIR.glob(\"**/*.txt\"))\n",
    "\n",
    "num_files = len(files)\n",
    "num_lines = 0\n",
    "num_chars = 0\n",
    "\n",
    "for p in tqdm(files, desc=\"scan files\"):\n",
    "    with p.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            num_lines += 1\n",
    "            num_chars += len(line)\n",
    "\n",
    "avg_chars_per_line = (num_chars / num_lines) if num_lines else 0.0\n",
    "avg_lines_per_file = (num_lines / num_files) if num_files else 0.0\n",
    "\n",
    "print({\n",
    "    \"num_files\": num_files,\n",
    "    \"num_lines\": num_lines,\n",
    "    \"num_chars\": num_chars,\n",
    "    \"avg_chars_per_line\": round(avg_chars_per_line, 2),\n",
    "    \"avg_lines_per_file\": round(avg_lines_per_file, 2),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all lines, strip trailing whitespace; group into paragraphs by blank lines\n",
    "all_lines: list[str] = []\n",
    "paragraphs: list[str] = []\n",
    "current_paragraph: list[str] = []\n",
    "\n",
    "for p in tqdm(files, desc=\"read paragraphs\"):\n",
    "    with p.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.rstrip(\"\\n\\r\")\n",
    "            all_lines.append(line)\n",
    "            if line.strip():\n",
    "                current_paragraph.append(line)\n",
    "            else:\n",
    "                if current_paragraph:\n",
    "                    paragraphs.append(\"\\n\".join(current_paragraph))\n",
    "                    current_paragraph = []\n",
    "        if current_paragraph:\n",
    "            paragraphs.append(\"\\n\".join(current_paragraph))\n",
    "            current_paragraph = []\n",
    "\n",
    "num_lines_before = len(all_lines)\n",
    "num_paragraphs_before = len(paragraphs)\n",
    "\n",
    "# Deduplicate exact lines and exact paragraphs\n",
    "unique_lines = list(dict.fromkeys(all_lines))\n",
    "unique_paragraphs = list(dict.fromkeys(paragraphs))\n",
    "\n",
    "print({\n",
    "    \"lines_before\": num_lines_before,\n",
    "    \"lines_after_unique\": len(unique_lines),\n",
    "    \"paragraphs_before\": num_paragraphs_before,\n",
    "    \"paragraphs_after_unique\": len(unique_paragraphs),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow punctuation, digits, whitespace, and Cyrillic letters (incl. Ё/ё). Filter out lines containing non-Cyrillic letters.\n",
    "ALLOWED_CHARS = set(\" \\t\\n\\r0123456789!?,.:;-—()[]{}'\\\"“…»«/\\\\|@#$%^&*+=~`<>·•–\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2009\\u2026\")\n",
    "\n",
    "def is_cyrillic_line(line: str) -> bool:\n",
    "    for ch in line:\n",
    "        if ch.isalpha():\n",
    "            if not (\"А\" <= ch <= \"Я\" or \"а\" <= ch <= \"я\" or ch in (\"Ё\", \"ё\")):\n",
    "                return False\n",
    "        elif not (ch in ALLOWED_CHARS or ch.isspace()):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "filtered_lines: list[str] = [ln for ln in unique_lines if is_cyrillic_line(ln)]\n",
    "print({\n",
    "    \"lines_before\": len(unique_lines),\n",
    "    \"lines_after_cyrillic_filter\": len(filtered_lines),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from filtered_lines; normalize repeated punctuation and whitespace.\n",
    "# Rules:\n",
    "# - Replace sequences of ! or ? with a single char.\n",
    "# - Normalize ellipsis variations to a single … (or three dots).\n",
    "# - Collapse multiple spaces/tabs to a single space (preserve newlines).\n",
    "# - Trim trailing spaces.\n",
    "\n",
    "norm_lines: list[str] = []\n",
    "for ln in tqdm(filtered_lines, desc=\"normalize\"):\n",
    "    s = ln\n",
    "    s = re.sub(r\"[!]{2,}\", \"!\", s)\n",
    "    s = re.sub(r\"[?]{2,}\", \"?\", s)\n",
    "    s = re.sub(r\"(\\.{3,}|…{2,})\", \"…\", s)\n",
    "    s = re.sub(r\"[ \\t]{2,}\", \" \", s)\n",
    "    s = s.rstrip()\n",
    "    norm_lines.append(s)\n",
    "\n",
    "print({\n",
    "    \"lines_before\": len(filtered_lines),\n",
    "    \"lines_after_norm\": len(norm_lines),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Char-based chunking removed; token-level packing will follow after tokenizer training.\n",
    "joined_text = []\n",
    "for para in tqdm(unique_paragraphs, desc=\"join paragraphs\"):\n",
    "    s = para.replace(\"\\r\", \"\").strip()\n",
    "    if s:\n",
    "        joined_text.append(s)\n",
    "\n",
    "print({\n",
    "    \"paragraphs_joined\": len(joined_text),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_stats = {\n",
    "    \"initial_files\": len(corpus_txt_files),\n",
    "    \"initial_lines\": num_lines_before,\n",
    "}\n",
    "\n",
    "post_stats = {\n",
    "    \"unique_lines\": len(unique_lines),\n",
    "    \"unique_paragraphs\": len(unique_paragraphs),\n",
    "    \"cyrillic_lines\": len(filtered_lines),\n",
    "    \"normalized_lines\": len(norm_lines),\n",
    "    \"paragraphs_joined\": len(joined_text),\n",
    "}\n",
    "\n",
    "print(\"Preprocessing summary:\\n\" + json.dumps({\"pre\": pre_stats, \"post\": post_stats}, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "VOCAB_SIZE = 3000\n",
    "\n",
    "# Train on normalized lines as corpus\n",
    "trainer = BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=SPECIAL_TOKENS)\n",
    "\n",
    "model = BPE(unk_token=\"<unk>\")\n",
    "tokenizer = Tokenizer(model)\n",
    "setattr(tokenizer, \"pre_tokenizer\", Whitespace())\n",
    "\n",
    "tokenizer.train_from_iterator(norm_lines, trainer=trainer)\n",
    "\n",
    "# Add BOS/EOS processing\n",
    "setattr(tokenizer, \"post_processor\", TemplateProcessing(\n",
    "    single=\"<bos> $A <eos>\",\n",
    "    pair=\"<bos> $A <eos> <bos> $B <eos>\",\n",
    "    special_tokens=[\n",
    "        (\"<bos>\", tokenizer.token_to_id(\"<bos>\")),\n",
    "        (\"<eos>\", tokenizer.token_to_id(\"<eos>\")),\n",
    "    ],\n",
    "))\n",
    "\n",
    "print({\n",
    "    \"vocab_size\": tokenizer.get_vocab_size(),\n",
    "    \"bos_id\": tokenizer.token_to_id(\"<bos>\"),\n",
    "    \"eos_id\": tokenizer.token_to_id(\"<eos>\"),\n",
    "    \"pad_id\": tokenizer.token_to_id(\"<pad>\"),\n",
    "    \"unk_id\": tokenizer.token_to_id(\"<unk>\"),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_PATH = Path(\"tokenizer.json\")\n",
    "\n",
    "tokenizer.save(str(TOKENIZER_PATH))\n",
    "print({\"saved\": TOKENIZER_PATH.exists(), \"path\": str(TOKENIZER_PATH)})\n",
    "\n",
    "reloaded = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
    "print({\n",
    "    \"vocab_size\": reloaded.get_vocab_size(),\n",
    "    \"bos_id\": reloaded.token_to_id(\"<bos>\"),\n",
    "    \"eos_id\": reloaded.token_to_id(\"<eos>\"),\n",
    "    \"pad_id\": reloaded.token_to_id(\"<pad>\"),\n",
    "    \"unk_id\": reloaded.token_to_id(\"<unk>\"),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round-trip encode/decode verification using the reloaded tokenizer\n",
    "# Use a non-empty line from the corpus-derived normalized lines to avoid embedding literals\n",
    "sample = next((s for s in norm_lines if s.strip()), \"Sample text.\")\n",
    "enc = reloaded.encode(sample)\n",
    "dec = reloaded.decode(enc.ids)\n",
    "\n",
    "print({\n",
    "    \"sample_preview\": sample[:80],\n",
    "    \"num_ids\": len(enc.ids),\n",
    "    \"decoded_equals\": dec == sample,\n",
    "})\n",
    "\n",
    "assert dec == sample, \"Round-trip encode/decode mismatch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = reloaded.token_to_id(\"<pad>\")\n",
    "BOS_ID = reloaded.token_to_id(\"<bos>\")\n",
    "EOS_ID = reloaded.token_to_id(\"<eos>\")\n",
    "UNK_ID = reloaded.token_to_id(\"<unk>\")\n",
    "\n",
    "print({\n",
    "    \"PAD_ID\": PAD_ID,\n",
    "    \"BOS_ID\": BOS_ID,\n",
    "    \"EOS_ID\": EOS_ID,\n",
    "    \"UNK_ID\": UNK_ID,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token-level packing to 512 tokens (BOS/EOS already applied by post-processor)\n",
    "MAX_LEN = 512\n",
    "ids_stream: List[int] = []\n",
    "for t in tqdm(joined_text, desc=\"encode paragraphs\"):\n",
    "    e = reloaded.encode(t)\n",
    "    ids_stream.extend(e.ids)\n",
    "\n",
    "input_ids: List[List[int]] = []\n",
    "attention_mask: List[List[int]] = []\n",
    "cur: List[int] = []\n",
    "for tok in ids_stream:\n",
    "    cur.append(tok)\n",
    "    while len(cur) >= MAX_LEN:\n",
    "        input_ids.append(cur[:MAX_LEN])\n",
    "        attention_mask.append([1] * MAX_LEN)\n",
    "        cur = cur[MAX_LEN:]\n",
    "if cur:\n",
    "    input_ids.append(cur)\n",
    "    attention_mask.append([1] * len(cur))\n",
    "\n",
    "train_ds = Dataset.from_dict({\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\": attention_mask,\n",
    "})\n",
    "\n",
    "print({\n",
    "    \"num_sequences\": len(train_ds),\n",
    "    \"avg_len\": round(sum(len(x) for x in input_ids) / max(1, len(input_ids)), 1),\n",
    "    \"max_len\": max((len(x) for x in input_ids), default=0),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic 95/5 split by index\n",
    "split_idx = int(0.95 * len(train_ds))\n",
    "train_split = train_ds.select(range(0, split_idx))\n",
    "val_split = train_ds.select(range(split_idx, len(train_ds)))\n",
    "\n",
    "datasets_dict = DatasetDict({\n",
    "    \"train\": train_split,\n",
    "    \"validation\": val_split,\n",
    "})\n",
    "\n",
    "print({k: len(v) for k, v in datasets_dict.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a HF fast tokenizer from tokenizer.json for Trainer compatibility\n",
    "tok_fast = PreTrainedTokenizerFast(tokenizer_file=str(TOKENIZER_PATH))\n",
    "tok_fast.pad_token = \"<pad>\"\n",
    "tok_fast.unk_token = \"<unk>\"\n",
    "tok_fast.bos_token = \"<bos>\"\n",
    "tok_fast.eos_token = \"<eos>\"\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tok_fast,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Quick sanity: collate a tiny batch\n",
    "batch = data_collator([train_ds[0], train_ds[min(1, len(train_ds)-1)]])\n",
    "print({k: (v.shape if hasattr(v, 'shape') else type(v)) for k, v in batch.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "VOCAB_SIZE = reloaded.get_vocab_size()\n",
    "\n",
    "config = LlamaConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=1536,\n",
    "    num_hidden_layers=16,\n",
    "    num_attention_heads=16,\n",
    "    num_key_value_heads=8,\n",
    "    max_position_embeddings=512,\n",
    "    pad_token_id=PAD_ID,\n",
    "    bos_token_id=BOS_ID,\n",
    "    eos_token_id=EOS_ID,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM(config)\n",
    "# Ensure embeddings/head are tied and padding idx set\n",
    "model.resize_token_embeddings(VOCAB_SIZE)\n",
    "model.get_input_embeddings().padding_idx = PAD_ID\n",
    "model.tie_weights()\n",
    "\n",
    "# Quick checks\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "has_output_head = model.get_output_embeddings() is not None\n",
    "print({\n",
    "    \"num_params\": num_params,\n",
    "    \"has_output_head\": has_output_head,\n",
    "    \"pad_token_id\": model.config.pad_token_id,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Effective batch size target 64–128 via gradient accumulation\n",
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = 32  # adjust based on VRAM to hit effective batch size\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"outputs/pretrain\",\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    num_train_epochs=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    bf16=False,\n",
    "    logging_steps=50,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=datasets_dict[\"train\"],\n",
    "    eval_dataset=datasets_dict[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"trainer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 fixed pretrain prompts from assignment\n",
    "test_prompts = [\n",
    "    \"Все мысли, которые имеют огромные последствия\",\n",
    "    \"Сила войска зависит от его духа\",\n",
    "    \"Мысль о том, что он принес страдания\",\n",
    "    \"Человек сознает себя свободным\",\n",
    "    \"Что бы ни случилось, я всегда буду\",\n",
    "    \"Любовь мешает смерти\",\n",
    "    \"Нет, жизнь не кончена\",\n",
    "    \"Всякая мысль, даже самая простая\",\n",
    "    \"Война не любезность, а самое гадкое дело\",\n",
    "    \"Чтобы жить честно\",\n",
    "]\n",
    "\n",
    "# Minimal epoch-end generation callback\n",
    "class EpochGenCB(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, model=None, tokenizer=None, **kwargs):[override]\n",
    "        gens = []\n",
    "        gen_kwargs = dict(max_new_tokens=64, do_sample=False, pad_token_id=PAD_ID, eos_token_id=EOS_ID)\n",
    "        for p in test_prompts:\n",
    "            x = tok_fast([p], return_tensors=\"pt\").to(model.device)\n",
    "            with torch.inference_mode():\n",
    "                y = model.generate(**x, **gen_kwargs)\n",
    "            gens.append({\"prompt\": p, \"text\": tok_fast.decode(y[0], skip_special_tokens=True)})\n",
    "        epoch_generations.append({\"epoch\": int(state.epoch), \"generations\": gens})\n",
    "        print(f\"Stored generations for epoch {int(state.epoch)}\")\n",
    "\n",
    "epoch_generations: list[dict] = []\n",
    "trainer.add_callback(EpochGenCB())\n",
    "print(\"epoch-end generation callback registered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "train_result = trainer.train()\n",
    "metrics_train = train_result.metrics\n",
    "\n",
    "# Evaluate to get validation loss\n",
    "metrics_eval = trainer.evaluate()\n",
    "val_loss = metrics_eval.get(\"eval_loss\", None)\n",
    "val_ppl = (exp(val_loss) if val_loss is not None else None)\n",
    "\n",
    "print({\n",
    "    \"train_metrics\": {k: float(v) for k, v in metrics_train.items() if isinstance(v, (int, float))},\n",
    "    \"eval_metrics\": {k: float(v) for k, v in metrics_eval.items() if isinstance(v, (int, float))},\n",
    "    \"val_ppl\": (float(val_ppl) if val_ppl is not None else None),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "SAVE_DIR = Path(\"outputs/pretrain/final\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "trainer.save_model(str(SAVE_DIR))  # saves model + tokenizer (tok_fast)\n",
    "trainer.state.save_to_json(str(SAVE_DIR / \"trainer_state.json\"))\n",
    "\n",
    "print({\n",
    "    \"saved_dir\": str(SAVE_DIR),\n",
    "    \"exists\": SAVE_DIR.exists(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca = load_dataset(\"parquet\", data_files=str(ALPACA_DIR / \"train-00000-of-00001-*.parquet\"))\n",
    "print(alpaca)\n",
    "train_split = alpaca[\"train\"]\n",
    "num_rows = int(getattr(train_split, \"num_rows\", 0))\n",
    "columns = list(getattr(getattr(train_split, \"features\", {}), \"keys\", lambda: [])())\n",
    "print({\n",
    "    \"num_rows\": num_rows,\n",
    "    \"columns\": columns,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map fields strictly: input→system, instruction→user, output→assistant (no pandas)\n",
    "sft_train = alpaca[\"train\"]\n",
    "feat = getattr(sft_train, \"features\", None)\n",
    "columns = list(feat.keys()) if feat is not None else list(getattr(sft_train, \"column_names\", []))\n",
    "required = {\"input\", \"instruction\", \"output\"}\n",
    "assert required.issubset(set(columns)), f\"Missing required columns: {required - set(columns)}\"\n",
    "\n",
    "def _map(row):\n",
    "    return {\n",
    "        \"system\": (row.get(\"input\") or \"\"),\n",
    "        \"user\": (row.get(\"instruction\") or \"\"),\n",
    "        \"assistant\": (row.get(\"output\") or \"\"),\n",
    "    }\n",
    "\n",
    "sft_mapped = sft_train.map(_map, remove_columns=sft_train.column_names)\n",
    "\n",
    "print({\n",
    "    \"num_rows\": int(getattr(sft_mapped, \"num_rows\", 0)),\n",
    "    \"columns\": list(getattr(sft_mapped, \"features\", {}).keys()),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QWEN_ID = \"Qwen/Qwen2.5-0.5B\"\n",
    "qwen_tok = AutoTokenizer.from_pretrained(QWEN_ID)\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(QWEN_ID, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "\n",
    "# Ensure pad and eos are set\n",
    "if qwen_tok.pad_token is None:\n",
    "    qwen_tok.pad_token = qwen_tok.eos_token or \"<|endoftext|>\"\n",
    "\n",
    "print({\n",
    "    \"pad_token\": qwen_tok.pad_token,\n",
    "    \"pad_token_id\": qwen_tok.pad_token_id,\n",
    "    \"eos_token\": qwen_tok.eos_token,\n",
    "    \"eos_token_id\": qwen_tok.eos_token_id,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build conversational texts via tokenizer chat template (no pandas)\n",
    "messages_texts = []\n",
    "for r in tqdm(sft_mapped, desc=\"build sft texts\"):\n",
    "    messages = []\n",
    "    sys = (r.get(\"system\") or \"\").strip()\n",
    "    if sys:\n",
    "        messages.append({\"role\": \"system\", \"content\": sys})\n",
    "    messages.append({\"role\": \"user\", \"content\": r.get(\"user\") or \"\"})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": r.get(\"assistant\") or \"\"})\n",
    "    text = qwen_tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    messages_texts.append(text)\n",
    "\n",
    "sft_text_ds = Dataset.from_dict({\"text\": messages_texts})\n",
    "# Small validation split\n",
    "sft_splits = sft_text_ds.train_test_split(test_size=0.05, seed=SEED)\n",
    "\n",
    "sft_args = TrainingArguments(\n",
    "    output_dir=\"outputs/sft\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=32,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.0,\n",
    "    warmup_ratio=0.03,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=50,\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"no\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=qwen_model,\n",
    "    tokenizer=qwen_tok,\n",
    "    args=sft_args,\n",
    "    train_dataset=sft_splits[\"train\"],\n",
    "    eval_dataset=sft_splits[\"test\"],\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "print(\"sft trainer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_train_result = sft_trainer.train()\n",
    "print({k: float(v) for k, v in sft_train_result.metrics.items() if isinstance(v, (int, float))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answers for the 4 evaluation questions\n",
    "questions_rus = [\n",
    "    \"сколько планет в нашей солнечной системе?\",\n",
    "    \"расскажи стих\",\n",
    "    \"когда собирать крыжовник?\",\n",
    "    \"Как быстро выучить новый язык?\",\n",
    "]\n",
    "\n",
    "responses = []\n",
    "for q in questions_rus:\n",
    "    msgs = []\n",
    "    if \"\" != \"\":\n",
    "        msgs.append({\"role\": \"system\", \"content\": \"\"})\n",
    "    msgs.append({\"role\": \"user\", \"content\": q})\n",
    "    prompt = qwen_tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = qwen_tok([prompt], return_tensors=\"pt\").to(qwen_model.device)\n",
    "    with torch.inference_mode():\n",
    "        out = qwen_model.generate(**inputs, max_new_tokens=128, do_sample=False, pad_token_id=qwen_tok.pad_token_id, eos_token_id=qwen_tok.eos_token_id)\n",
    "    text = qwen_tok.decode(out[0], skip_special_tokens=True)\n",
    "    responses.append({\"question\": q, \"answer\": text})\n",
    "\n",
    "for r in responses:\n",
    "    print(\"Q:\", r[\"question\"]) \n",
    "    print(\"A:\", r[\"answer\"]) \n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SFT model and tokenizer\n",
    "from pathlib import Path\n",
    "SFT_DIR = Path(\"outputs/sft/final\")\n",
    "SFT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sft_trainer.save_model(str(SFT_DIR))\n",
    "qwen_tok.save_pretrained(str(SFT_DIR))\n",
    "\n",
    "print({\"sft_saved_dir\": str(SFT_DIR), \"exists\": SFT_DIR.exists()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "summary = {\n",
    "    \"seed\": SEED,\n",
    "    \"device\": str(device),\n",
    "    \"device_name\": device_name,\n",
    "    \"dtype\": str(dtype),\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"torch\": torch.__version__,\n",
    "    \"transformers\": __import__(\"transformers\").__version__,\n",
    "    \"datasets\": __import__(\"datasets\").__version__,\n",
    "    \"trl\": __import__(\"trl\").__version__,\n",
    "    \"pretrain\": {\n",
    "        \"vocab_size\": int(VOCAB_SIZE),\n",
    "        \"num_params\": int(sum(p.numel() for p in model.parameters())),\n",
    "        \"context_len\": int(model.config.max_position_embeddings),\n",
    "    },\n",
    "    \"sft\": {\n",
    "        \"base_model\": QWEN_ID,\n",
    "        \"train_rows\": int(getattr(sft_splits[\"train\"], \"num_rows\", 0)),\n",
    "        \"eval_rows\": int(getattr(sft_splits[\"test\"], \"num_rows\", 0)),\n",
    "    },\n",
    "}\n",
    "print(json.dumps(summary, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated displays: 10 pretrain generations (last epoch) and 4 SFT answers\n",
    "import json\n",
    "\n",
    "if epoch_generations:\n",
    "    last = epoch_generations[-1]\n",
    "    print(f\"Pretrain epoch {last['epoch']} generations:\")\n",
    "    for i, g in enumerate(last[\"generations\"], 1):\n",
    "        print(f\"[{i:02d}] prompt: {g['prompt']}\")\n",
    "        print(f\"     text: {g['text']}\")\n",
    "        print(\"---\")\n",
    "else:\n",
    "    print(\"No pretrain generations recorded.\")\n",
    "\n",
    "print(\"SFT evaluation answers:\")\n",
    "for r in responses:\n",
    "    print(\"Q:\", r[\"question\"]) \n",
    "    print(\"A:\", r[\"answer\"]) \n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist outputs to JSON/CSV\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path(\"outputs/reports\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Pretrain generations per epoch to JSON\n",
    "with (OUT_DIR / \"pretrain_epoch_generations.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(epoch_generations, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Last epoch generations to CSV\n",
    "with (OUT_DIR / \"pretrain_last_epoch_generations.csv\").open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"epoch\", \"idx\", \"prompt\", \"text\"])\n",
    "    if epoch_generations:\n",
    "        last = epoch_generations[-1]\n",
    "        for i, g in enumerate(last[\"generations\"], 1):\n",
    "            w.writerow([last[\"epoch\"], i, g[\"prompt\"], g[\"text\"]])\n",
    "\n",
    "# SFT responses to CSV\n",
    "with (OUT_DIR / \"sft_eval_responses.csv\").open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"question\", \"answer\"])\n",
    "    for r in responses:\n",
    "        w.writerow([r[\"question\"], r[\"answer\"]])\n",
    "\n",
    "print({\"written\": True, \"dir\": str(OUT_DIR)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
