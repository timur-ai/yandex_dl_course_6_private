{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEREL Joint NER + Document Classification (Multi-Task)\n",
    "\n",
    "This notebook implements a single-encoder model that performs token-level NER (BIO) and document-level multi-label classification on NEREL.\n",
    "\n",
    "- Encoder: Hugging Face Transformer (small Russian BERT)\n",
    "- Heads: token classification head (NER), doc classification head (multi-label)\n",
    "- Loss: sum or uncertainty-weighted\n",
    "- Metrics: token F1 (seqeval), doc micro-F1 (sklearn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and imports\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_NAME = os.environ.get(\"RU_BASE_MODEL\", \"cointegrated/rubert-tiny2\")\n",
    "USE_UNCERTAINTY = True\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 8\n",
    "LR = 2e-5\n",
    "EPOCHS = 3\n",
    "ACCUM_STEPS = 1\n",
    "GRAD_CLIP_NORM = 1.0\n",
    "\n",
    "DATA_DIR = \".\"\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, \"train.jsonl\")\n",
    "DEV_PATH = os.path.join(DATA_DIR, \"dev.jsonl\")\n",
    "TEST_PATH = os.path.join(DATA_DIR, \"test.jsonl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NEREL JSONL files if missing\n",
    "import urllib.request\n",
    "\n",
    "NEREL_BASE = \"https://huggingface.co/datasets/iluvvatar/NEREL/resolve/main/data\"\n",
    "FILES = {\n",
    "    TRAIN_PATH: f\"{NEREL_BASE}/train.jsonl\",\n",
    "    DEV_PATH: f\"{NEREL_BASE}/dev.jsonl\",\n",
    "    TEST_PATH: f\"{NEREL_BASE}/test.jsonl\",\n",
    "}\n",
    "\n",
    "for local_path, url in FILES.items():\n",
    "    if not os.path.exists(local_path):\n",
    "        urllib.request.urlretrieve(url, local_path)\n",
    "local_paths = list(FILES.keys())\n",
    "local_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data utils: loading JSONL and simple EDA helpers\n",
    "\n",
    "def read_jsonl(path: str) -> List[Dict[str, Any]]:\n",
    "    data: List[Dict[str, Any]] = []\n",
    "    if not os.path.exists(path):\n",
    "        return data\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "train_raw = read_jsonl(TRAIN_PATH)\n",
    "dev_raw = read_jsonl(DEV_PATH)\n",
    "test_raw = read_jsonl(TEST_PATH)\n",
    "\n",
    "len(train_raw), len(dev_raw), len(test_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: lightweight counts (safe if dataset not yet downloaded)\n",
    "from collections import Counter\n",
    "from typing import Counter as TCounter\n",
    "\n",
    "def top_k(counter: TCounter[Any], k: int = 15):\n",
    "    return counter.most_common(k)\n",
    "\n",
    "entity_type_counter: TCounter[str] = Counter()\n",
    "text_len_counter: TCounter[int] = Counter()\n",
    "entities_per_doc_counter: TCounter[int] = Counter()\n",
    "\n",
    "for doc in train_raw[:2000]:\n",
    "    text = doc.get(\"text\", \"\")\n",
    "    ents = doc.get(\"entities\", [])\n",
    "    for e in ents:\n",
    "        entity_type_counter[e.get(\"type\", \"UNKNOWN\")] += 1\n",
    "    text_len_counter[len(text.split())] += 1\n",
    "    entities_per_doc_counter[len(ents)] += 1\n",
    "\n",
    "{\n",
    "    \"top_entity_types\": top_k(entity_type_counter, 15),\n",
    "    \"text_len_bins\": top_k(text_len_counter, 15),\n",
    "    \"entities_per_doc\": top_k(entities_per_doc_counter, 15),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing utilities: whitespace tokens, spans, BIO, doc labels\n",
    "\n",
    "def whitespace_tokenize_with_spans(text: str) -> Tuple[List[str], List[Tuple[int, int]]]:\n",
    "    tokens: List[str] = []\n",
    "    spans: List[Tuple[int, int]] = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        if text[i].isspace():\n",
    "            i += 1\n",
    "            continue\n",
    "        j = i\n",
    "        while j < len(text) and not text[j].isspace():\n",
    "            j += 1\n",
    "        tokens.append(text[i:j])\n",
    "        spans.append((i, j))\n",
    "        i = j\n",
    "    return tokens, spans\n",
    "\n",
    "@dataclass(frozen=True, slots=True)\n",
    "class Entity:\n",
    "    start: int\n",
    "    end: int\n",
    "    type: str\n",
    "\n",
    "# Build label spaces\n",
    "\n",
    "def build_label_spaces(docs: List[Dict[str, Any]], top_k_events: int = 30):\n",
    "    ent_types: Dict[str, int] = {}\n",
    "    event_counter: Dict[str, int] = {}\n",
    "\n",
    "    for doc in docs:\n",
    "        for e in doc.get(\"entities\", []):\n",
    "            ent_types[e.get(\"type\", \"UNKNOWN\")] = 1\n",
    "        for rel in doc.get(\"relations\", []):\n",
    "            t = rel.get(\"type\", \"UNKNOWN\")\n",
    "            event_counter[t] = event_counter.get(t, 0) + 1\n",
    "        for ev in doc.get(\"events\", []):\n",
    "            t = ev.get(\"type\", \"UNKNOWN\")\n",
    "            event_counter[t] = event_counter.get(t, 0) + 1\n",
    "\n",
    "    ent_types_sorted = sorted(ent_types.keys())\n",
    "    ner_tags: List[str] = [\"O\"]\n",
    "    for t in ent_types_sorted:\n",
    "        ner_tags.append(f\"B-{t}\")\n",
    "        ner_tags.append(f\"I-{t}\")\n",
    "    ner_label_to_id = {t: i for i, t in enumerate(ner_tags)}\n",
    "    ner_id_to_label = {i: t for t, i in ner_label_to_id.items()}\n",
    "\n",
    "    event_sorted = sorted(event_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_events = [t for t, _ in event_sorted[:top_k_events]]\n",
    "    doc_label_to_id = {t: i for i, t in enumerate(top_events)}\n",
    "    doc_id_to_label = {i: t for t, i in doc_label_to_id.items()}\n",
    "\n",
    "    return ner_label_to_id, ner_id_to_label, doc_label_to_id, doc_id_to_label\n",
    "\n",
    "# Convert entities to BIO over whitespace tokens\n",
    "\n",
    "def entities_to_bio(tokens: List[str], spans: List[Tuple[int, int]], ents: List[Dict[str, Any]], ner_label_to_id: Dict[str, int]) -> List[int]:\n",
    "    labels = [ner_label_to_id[\"O\"]] * len(tokens)\n",
    "    for e in ents:\n",
    "        st = int(e.get(\"start\", -1))\n",
    "        en = int(e.get(\"end\", -1))\n",
    "        et = e.get(\"type\", \"UNKNOWN\")\n",
    "        began = False\n",
    "        for idx, (s, e_) in enumerate(spans):\n",
    "            if s >= en:\n",
    "                break\n",
    "            if e_ <= st:\n",
    "                continue\n",
    "            if s < en and e_ > st:\n",
    "                if not began:\n",
    "                    labels[idx] = ner_label_to_id.get(f\"B-{et}\", ner_label_to_id[\"O\"])\n",
    "                    began = True\n",
    "                else:\n",
    "                    labels[idx] = ner_label_to_id.get(f\"I-{et}\", ner_label_to_id[\"O\"])\n",
    "    return labels\n",
    "\n",
    "# Build examples from raw docs\n",
    "\n",
    "def build_examples_from_nerel(raw_docs: List[Dict[str, Any]], ner_label_to_id: Dict[str, int], doc_label_to_id: Dict[str, int]):\n",
    "    examples = []\n",
    "    for doc in raw_docs:\n",
    "        text = doc.get(\"text\", \"\")\n",
    "        tokens, spans = whitespace_tokenize_with_spans(text)\n",
    "        bio = entities_to_bio(tokens, spans, doc.get(\"entities\", []), ner_label_to_id)\n",
    "        multi = np.zeros(len(doc_label_to_id), dtype=np.float32)\n",
    "        for rel in doc.get(\"relations\", []):\n",
    "            t = rel.get(\"type\", \"UNKNOWN\")\n",
    "            if t in doc_label_to_id:\n",
    "                multi[doc_label_to_id[t]] = 1.0\n",
    "        for ev in doc.get(\"events\", []):\n",
    "            t = ev.get(\"type\", \"UNKNOWN\")\n",
    "            if t in doc_label_to_id:\n",
    "                multi[doc_label_to_id[t]] = 1.0\n",
    "        examples.append({\n",
    "            \"tokens\": tokens,\n",
    "            \"token_spans\": spans,\n",
    "            \"bio\": bio,\n",
    "            \"cls_vec\": multi,\n",
    "        })\n",
    "    return examples\n",
    "\n",
    "# Label spaces\n",
    "ner_label_to_id, ner_id_to_label, doc_label_to_id, doc_id_to_label = build_label_spaces(train_raw)\n",
    "len(ner_label_to_id), len(doc_label_to_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train/dev/test examples\n",
    "train_examples = build_examples_from_nerel(train_raw, ner_label_to_id, doc_label_to_id)\n",
    "dev_examples = build_examples_from_nerel(dev_raw, ner_label_to_id, doc_label_to_id)\n",
    "test_examples = build_examples_from_nerel(test_raw, ner_label_to_id, doc_label_to_id)\n",
    "\n",
    "len(train_examples), len(dev_examples), len(test_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset fix and simple collate\n",
    "class JointDataset(Dataset):\n",
    "    def __init__(self, examples: List[Dict[str, Any]]):\n",
    "        super().__init__()\n",
    "        self.examples = examples\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.examples)\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        ex = self.examples[idx]\n",
    "        enc, lab = tokenize_and_align_labels(ex[\"tokens\"], ex[\"bio\"])\n",
    "        enc[\"labels\"] = lab\n",
    "        enc[\"cls_labels\"] = ex[\"cls_vec\"].astype(np.float32)\n",
    "        return enc\n",
    "\n",
    "train_ds = JointDataset(train_examples)\n",
    "dev_ds = JointDataset(dev_examples)\n",
    "test_ds = JointDataset(test_examples)\n",
    "\n",
    "\n",
    "def collate_batch(features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "    max_len = max(len(f[\"input_ids\"]) for f in features)\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    for f in features:\n",
    "        ids = f[\"input_ids\"]\n",
    "        att = f[\"attention_mask\"]\n",
    "        lab = f[\"labels\"]\n",
    "        pad_len = max_len - len(ids)\n",
    "        input_ids.append(ids + [tokenizer.pad_token_id] * pad_len)\n",
    "        attention_mask.append(att + [0] * pad_len)\n",
    "        labels.append(lab + [-100] * pad_len)\n",
    "    batch = {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        \"cls_labels\": torch.tensor([f[\"cls_labels\"] for f in features], dtype=torch.float32),\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "dev_loader = DataLoader(dev_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint model: encoder + two heads + optional uncertainty weighting\n",
    "class JointModel(nn.Module):\n",
    "    def __init__(self, base_model_name: str, num_ner_labels: int, num_doc_labels: int, use_uncertainty: bool = True):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(base_model_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.token_cls = nn.Linear(hidden, num_ner_labels)\n",
    "        self.cls_cls = nn.Linear(hidden, num_doc_labels)\n",
    "        self.use_uncertainty = use_uncertainty\n",
    "        if use_uncertainty:\n",
    "            self.log_sigma_token = nn.Parameter(torch.tensor(0.0))\n",
    "            self.log_sigma_cls = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        seq = self.dropout(outputs.last_hidden_state)\n",
    "        token_logits = self.token_cls(seq)\n",
    "        if hasattr(outputs, \"pooler_output\") and outputs.pooler_output is not None:\n",
    "            pooled = outputs.pooler_output\n",
    "        else:\n",
    "            pooled = seq[:, 0]\n",
    "        pooled = self.dropout(pooled)\n",
    "        cls_logits = self.cls_cls(pooled)\n",
    "        return {\"token_logits\": token_logits, \"cls_logits\": cls_logits}\n",
    "\n",
    "    def compute_loss(self, token_logits: torch.Tensor, token_labels: torch.Tensor, cls_logits: torch.Tensor, cls_labels: torch.Tensor) -> torch.Tensor:\n",
    "        ce = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        bce = nn.BCEWithLogitsLoss()\n",
    "        C = token_logits.size(-1)\n",
    "        token_loss = ce(token_logits.view(-1, C), token_labels.view(-1))\n",
    "        cls_loss = bce(cls_logits, cls_labels)\n",
    "        if getattr(self, \"use_uncertainty\", False):\n",
    "            loss_token_term = torch.exp(-2.0 * self.log_sigma_token) * token_loss + self.log_sigma_token\n",
    "            loss_cls_term = torch.exp(-2.0 * self.log_sigma_cls) * cls_loss + self.log_sigma_cls\n",
    "            return loss_token_term + loss_cls_term\n",
    "        return token_loss + cls_loss\n",
    "\n",
    "num_ner_labels = len(ner_label_to_id)\n",
    "num_doc_labels = len(doc_label_to_id)\n",
    "model = JointModel(MODEL_NAME, num_ner_labels, num_doc_labels, use_uncertainty=USE_UNCERTAINTY).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & evaluation utilities\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "\n",
    "def decode_token_preds(logits: np.ndarray, labels: np.ndarray) -> Tuple[List[str], List[str]]:\n",
    "    # Convert per-token logits and labels to label strings for F1\n",
    "    pred_ids = logits.argmax(-1)\n",
    "    y_true, y_pred = [], []\n",
    "    for true_row, pred_row in zip(labels, pred_ids):\n",
    "        for t, p in zip(true_row, pred_row):\n",
    "            if t == -100:\n",
    "                continue\n",
    "            y_true.append(ner_id_to_label[int(t)])\n",
    "            y_pred.append(ner_id_to_label[int(p)])\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def evaluate(model: JointModel, loader: DataLoader) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    all_true_doc_bin: List[int] = []\n",
    "    all_pred_doc_bin: List[int] = []\n",
    "\n",
    "    token_true_labels: List[str] = []\n",
    "    token_pred_labels: List[str] = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            cls_labels = batch[\"cls_labels\"].to(device)\n",
    "\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            token_logits = out[\"token_logits\"].detach().cpu().numpy()\n",
    "            cls_logits = out[\"cls_logits\"].detach().cpu().numpy()\n",
    "\n",
    "            y_true_tokens, y_pred_tokens = decode_token_preds(token_logits, labels.detach().cpu().numpy())\n",
    "            token_true_labels.extend(y_true_tokens)\n",
    "            token_pred_labels.extend(y_pred_tokens)\n",
    "\n",
    "            y_true_doc = (cls_labels.detach().cpu().numpy() > 0.5).astype(int)\n",
    "            y_pred_doc = (1 / (1 + np.exp(-cls_logits)) >= 0.5).astype(int)\n",
    "            all_true_doc_bin.extend(y_true_doc.reshape(-1).tolist())\n",
    "            all_pred_doc_bin.extend(y_pred_doc.reshape(-1).tolist())\n",
    "\n",
    "    token_f1 = f1_score(token_true_labels, token_pred_labels, average=\"macro\") if token_true_labels else 0.0\n",
    "    micro_f1 = f1_score(all_true_doc_bin, all_pred_doc_bin, average=\"micro\") if all_true_doc_bin else 0.0\n",
    "    micro_p = precision_score(all_true_doc_bin, all_pred_doc_bin, average=\"micro\") if all_true_doc_bin else 0.0\n",
    "    micro_r = recall_score(all_true_doc_bin, all_pred_doc_bin, average=\"micro\") if all_true_doc_bin else 0.0\n",
    "    return {\"token_f1\": float(token_f1), \"cls_micro_f1\": float(micro_f1), \"cls_precision\": float(micro_p), \"cls_recall\": float(micro_r)}\n",
    "\n",
    "\n",
    "def train(model: JointModel, train_loader: DataLoader, dev_loader: DataLoader, epochs: int = EPOCHS):\n",
    "    best_dev = -1.0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        step = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            cls_labels = batch[\"cls_labels\"].to(device)\n",
    "\n",
    "            with autocast(device_type=\"cuda\", dtype=torch.float16, enabled=torch.cuda.is_available()):\n",
    "                out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = model.compute_loss(out[\"token_logits\"], labels, out[\"cls_logits\"], cls_labels)\n",
    "                loss = loss / ACCUM_STEPS\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % ACCUM_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            step += 1\n",
    "\n",
    "        dev_metrics = evaluate(model, dev_loader)\n",
    "        print({\"epoch\": epoch + 1, **dev_metrics})\n",
    "        if dev_metrics[\"cls_micro_f1\"] > best_dev:\n",
    "            best_dev = dev_metrics[\"cls_micro_f1\"]\n",
    "    return best_dev\n",
    "\n",
    "best_dev = train(model, train_loader, dev_loader, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation on test set\n",
    "metrics_test = evaluate(model, test_loader)\n",
    "metrics_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "\n",
    "def predict(text: str) -> Dict[str, Any]:\n",
    "    model.eval()\n",
    "    tokens, spans = whitespace_tokenize_with_spans(text)\n",
    "    enc, lab = tokenize_and_align_labels(tokens, [ner_label_to_id[\"O\"]] * len(tokens))\n",
    "    batch = collate_batch([{**enc, \"labels\": lab, \"cls_labels\": np.zeros(len(doc_label_to_id), dtype=np.float32)}])\n",
    "    with torch.inference_mode():\n",
    "        out = model(batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device))\n",
    "        token_logits = out[\"token_logits\"][0].detach().cpu().numpy()\n",
    "        cls_logits = out[\"cls_logits\"][0].detach().cpu().numpy()\n",
    "    token_ids = token_logits.argmax(-1).tolist()\n",
    "    token_labels = [ner_id_to_label[i] if lab_i != -100 else \"IGN\" for i, lab_i in zip(token_ids, batch[\"labels\"][0].tolist())]\n",
    "    cls_probs = (1 / (1 + np.exp(-cls_logits))).tolist()\n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"token_labels\": token_labels,\n",
    "        \"cls\": sorted([(doc_id_to_label[i], float(p)) for i, p in enumerate(cls_probs)], key=lambda x: x[1], reverse=True)[:10],\n",
    "    }\n",
    "\n",
    "# Example\n",
    "predict(\"Компания А подписала договор с Банком Б 2020 года.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple dynamic quantization demo (CPU only)\n",
    "qm = torch.quantization.quantize_dynamic(model.cpu(), {nn.Linear}, dtype=torch.qint8)\n",
    "qm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and align labels\n",
    "\n",
    "def tokenize_and_align_labels(tokens: List[str], bio_labels: List[int]):\n",
    "    encoding = tokenizer(\n",
    "        tokens,\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    word_ids = encoding.word_ids()\n",
    "    labels: List[int] = []\n",
    "    for wi in word_ids:\n",
    "        if wi is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            labels.append(bio_labels[wi])\n",
    "    return encoding, labels\n",
    "\n",
    "class JointDataset(Dataset):\n",
    "    def __init__(self, examples: List[Dict[str, Any]]):\n",
    "        self.examples = examples\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.examples)\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        ex = self.examples[idx]\n",
    "        enc, lab = tokenize_and_align_labels(ex[\"tokens\"], ex[\"bio\"])\n",
    "        enc[\"labels\"] = lab\n",
    "        enc[\"cls_labels\"] = ex[\"cls_vec\"].astype(np.float32)\n",
    "        return enc\n",
    "\n",
    "train_ds = JointDataset(train_examples)\n",
    "dev_ds = JointDataset(dev_examples)\n",
    "test_ds = JointDataset(test_examples)\n",
    "\n",
    "collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
