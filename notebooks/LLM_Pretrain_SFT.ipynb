{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports all libraries used across the notebook for both stages:\n",
    "- Preprocess and analyze the Russian literature corpus (dedup, filtering, normalization)\n",
    "- Train a small BPE tokenizer (~3k vocab) with BOS/EOS\n",
    "- Build token-packed datasets for pretrain (context 512)\n",
    "- Define and train a ~150M decoder-only model with Trainer and epoch-end generations\n",
    "- Load Alpaca RU, format dialog data, and run SFT with Qwen2.5-0.5B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "# Disable hf-xet early (before any hub/transformers imports)\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "# Ensure hf_transfer is not forced unless explicitly desired\n",
    "os.environ.pop(\"HF_HUB_ENABLE_HF_TRANSFER\", None)\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from math import exp\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    LlamaConfig,\n",
    "    LlamaForCausalLM,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from trl.trainer.sft_trainer import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeding and device/dtype selection makes training deterministic and aligned with available hardware. Supports stable Trainer and SFT runs and reproducible metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"seed\": 42,\n",
      "  \"device\": \"cuda\",\n",
      "  \"device_name\": \"NVIDIA A100-PCIE-40GB\",\n",
      "  \"cuda\": true,\n",
      "  \"dtype\": \"torch.float16\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "SEED = int(os.environ.get(\"SEED\", \"42\"))\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "set_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_name = (\n",
    "    torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    ")\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"seed\": SEED,\n",
    "            \"device\": str(device),\n",
    "            \"device_name\": device_name,\n",
    "            \"cuda\": torch.cuda.is_available(),\n",
    "            \"dtype\": str(dtype),\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifies presence of corpus text files and Alpaca RU parquet shards before preprocessing and SFT. Ensures the full dataset scope is available and prevents wasted work later. Enumerates files for traceability and quick inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"corpus_num_files\": 108,\n",
      "  \"corpus_sample\": [\n",
      "    \"/home/user/code/yandex_dl_course_6_private/data/corpus/Bulgakov_BelayaGvardiya.txt\",\n",
      "    \"/home/user/code/yandex_dl_course_6_private/data/corpus/Bulgakov_Diavoliada.txt\",\n",
      "    \"/home/user/code/yandex_dl_course_6_private/data/corpus/Bulgakov_Master.txt\"\n",
      "  ],\n",
      "  \"alpaca_parquet_files\": [\n",
      "    \"/home/user/code/yandex_dl_course_6_private/data/alpaca-cleaned-ru/train-00000-of-00001-c503683bee003a5c.parquet\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ROOT = Path.cwd()\n",
    "if not (ROOT / \"data\").is_dir():\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "CORPUS_DIR = ROOT / \"data\" / \"corpus\"\n",
    "ALPACA_DIR = ROOT / \"data\" / \"alpaca-cleaned-ru\"\n",
    "\n",
    "assert (\n",
    "    CORPUS_DIR.exists() and CORPUS_DIR.is_dir()\n",
    "), f\"Missing directory: {CORPUS_DIR}\"\n",
    "assert (\n",
    "    ALPACA_DIR.exists() and ALPACA_DIR.is_dir()\n",
    "), f\"Missing directory: {ALPACA_DIR}\"\n",
    "\n",
    "corpus_txt_files = sorted(CORPUS_DIR.rglob(\"*.txt\"))\n",
    "parquet_files = sorted(ALPACA_DIR.glob(\"*.parquet\"))\n",
    "\n",
    "stats = {\n",
    "    \"corpus_num_files\": len(corpus_txt_files),\n",
    "    \"corpus_sample\": [p.as_posix() for p in corpus_txt_files[:3]],\n",
    "    \"alpaca_parquet_files\": [p.as_posix() for p in parquet_files],\n",
    "}\n",
    "\n",
    "print(json.dumps(stats, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes basic statistics (file/line/char counts) on the raw corpus to inform preprocessing and act as a baseline. It does not transform data but supports verification that later deduplication and filtering have expected magnitude. Helps detect anomalies before tokenizer training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"num_files\": 108,\n",
      "  \"num_lines\": 329299,\n",
      "  \"num_chars\": 45348575,\n",
      "  \"avg_chars_per_line\": 137.71,\n",
      "  \"avg_lines_per_file\": 3049.06\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "files = sorted(CORPUS_DIR.rglob(\"*.txt\"))\n",
    "\n",
    "num_files = len(files)\n",
    "num_lines = 0\n",
    "num_chars = 0\n",
    "\n",
    "for p in files:\n",
    "    with p.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            num_lines += 1\n",
    "            num_chars += len(line)\n",
    "\n",
    "avg_chars_per_line = (num_chars / num_lines) if num_lines else 0.0\n",
    "avg_lines_per_file = (num_lines / num_files) if num_files else 0.0\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"num_files\": num_files,\n",
    "            \"num_lines\": num_lines,\n",
    "            \"num_chars\": num_chars,\n",
    "            \"avg_chars_per_line\": round(avg_chars_per_line, 2),\n",
    "            \"avg_lines_per_file\": round(avg_lines_per_file, 2),\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groups lines into paragraphs and removes duplicate lines and paragraphs. This step reduces redundancy and organizes the text into coherent units before tokenization-aware packing. It specifically handles the deduplication stage of preprocessing and logs before-and-after counts for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"lines_before\": 329299,\n",
      "  \"lines_after_unique\": 305870,\n",
      "  \"paragraphs_before\": 9518,\n",
      "  \"paragraphs_after_unique\": 7383\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Read all lines, strip trailing whitespace; group into paragraphs by blank lines\n",
    "all_lines: list[str] = []\n",
    "paragraphs: list[str] = []\n",
    "current_paragraph: list[str] = []\n",
    "\n",
    "for p in files:\n",
    "    with p.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.rstrip(\"\\n\\r\")\n",
    "            all_lines.append(line)\n",
    "            if line.strip():\n",
    "                current_paragraph.append(line)\n",
    "            else:\n",
    "                if current_paragraph:\n",
    "                    paragraphs.append(\"\\n\".join(current_paragraph))\n",
    "                    current_paragraph = []\n",
    "        if current_paragraph:\n",
    "            paragraphs.append(\"\\n\".join(current_paragraph))\n",
    "            current_paragraph = []\n",
    "\n",
    "num_lines_before = len(all_lines)\n",
    "num_paragraphs_before = len(paragraphs)\n",
    "\n",
    "# Deduplicate exact lines and exact paragraphs\n",
    "unique_lines = list(dict.fromkeys(all_lines))\n",
    "unique_paragraphs = list(dict.fromkeys(paragraphs))\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"lines_before\": num_lines_before,\n",
    "            \"lines_after_unique\": len(unique_lines),\n",
    "            \"paragraphs_before\": num_paragraphs_before,\n",
    "            \"paragraphs_after_unique\": len(unique_paragraphs),\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enforces a Cyrillic-only constraint by rejecting lines containing non-Cyrillic letters (keeping punctuation/digits). This keeps the domain consistent with Russian literature and removes alphabetic noise. Improves the quality of the corpus used for tokenizer training and LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"lines_before\": 305870,\n",
      "  \"lines_after_cyrillic_filter\": 288461\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Allow punctuation, digits, whitespace, and Cyrillic letters (incl. Ё/ё). Filter out lines containing non-Cyrillic letters.\n",
    "ALLOWED_CHARS = set(\n",
    "    \" \\t\\n\\r0123456789!?,.:;-—()[]{}'\\\"“…»«/\\\\|@#$%^&*+=~`<>·•–\\u00a0\\u2000\\u2001\\u2002\\u2003\\u2009\\u2026\"\n",
    ")\n",
    "\n",
    "\n",
    "def is_cyrillic_line(line: str) -> bool:\n",
    "    for ch in line:\n",
    "        if ch.isalpha():\n",
    "            if not (\"А\" <= ch <= \"Я\" or \"а\" <= ch <= \"я\" or ch in (\"Ё\", \"ё\")):\n",
    "                return False\n",
    "        elif not (ch in ALLOWED_CHARS or ch.isspace()):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "filtered_lines: list[str] = [ln for ln in unique_lines if is_cyrillic_line(ln)]\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"lines_before\": len(unique_lines),\n",
    "            \"lines_after_cyrillic_filter\": len(filtered_lines),\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizes repeated punctuation and collapses excessive whitespace, including ellipsis variants. This reduces superficial variance and improves tokenizer training quality on domain text while keeping meaning intact. Standardization here supports more stable tokenization statistics used for BPE learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"lines_before\": 288461,\n",
      "  \"lines_after_norm\": 288461\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Start from filtered_lines; normalize repeated punctuation and whitespace.\n",
    "# Rules:\n",
    "# - Replace sequences of ! or ? with a single char.\n",
    "# - Normalize ellipsis variations to a single … (or three dots).\n",
    "# - Collapse multiple spaces/tabs to a single space (preserve newlines).\n",
    "# - Trim trailing spaces.\n",
    "\n",
    "norm_lines: list[str] = []\n",
    "for ln in filtered_lines:\n",
    "    s = ln\n",
    "    s = re.sub(r\"[!]{2,}\", \"!\", s)\n",
    "    s = re.sub(r\"[?]{2,}\", \"?\", s)\n",
    "    s = re.sub(r\"(\\.{3,}|…{2,})\", \"…\", s)\n",
    "    s = re.sub(r\"[ \\t]{2,}\", \" \", s)\n",
    "    s = s.rstrip()\n",
    "    norm_lines.append(s)\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"lines_before\": len(filtered_lines),\n",
    "            \"lines_after_norm\": len(norm_lines),\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produces cleaned paragraph strings and defers chunking to token level, which will occur after tokenizer training. This aligns with preparing for context-length boundaries with BOS/EOS later. It removes stray carriage returns and trims text, ensuring packable units and a deterministic pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"paragraphs_joined\": 7383\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Char-based chunking removed; token-level packing will follow after tokenizer training.\n",
    "joined_text = []\n",
    "for para in unique_paragraphs:\n",
    "    s = para.replace(\"\\r\", \"\").strip()\n",
    "    if s:\n",
    "        joined_text.append(s)\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"paragraphs_joined\": len(joined_text),\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints a pre/post summary of key counts (unique lines/paragraphs, filtered and normalized lines, joined paragraphs). This validates that deduplication and filtering had the intended effect and supports traceability before tokenizer training. It is a lightweight audit to catch anomalies early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing summary:\n",
      "{\n",
      "  \"pre\": {\n",
      "    \"initial_files\": 108,\n",
      "    \"initial_lines\": 329299\n",
      "  },\n",
      "  \"post\": {\n",
      "    \"unique_lines\": 305870,\n",
      "    \"unique_paragraphs\": 7383,\n",
      "    \"cyrillic_lines\": 288461,\n",
      "    \"normalized_lines\": 288461,\n",
      "    \"paragraphs_joined\": 7383\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "pre_stats = {\n",
    "    \"initial_files\": len(corpus_txt_files),\n",
    "    \"initial_lines\": num_lines_before,\n",
    "}\n",
    "\n",
    "post_stats = {\n",
    "    \"unique_lines\": len(unique_lines),\n",
    "    \"unique_paragraphs\": len(unique_paragraphs),\n",
    "    \"cyrillic_lines\": len(filtered_lines),\n",
    "    \"normalized_lines\": len(norm_lines),\n",
    "    \"paragraphs_joined\": len(joined_text),\n",
    "}\n",
    "\n",
    "print(\n",
    "    \"Preprocessing summary:\\n\"\n",
    "    + json.dumps(\n",
    "        {\"pre\": pre_stats, \"post\": post_stats}, ensure_ascii=False, indent=2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trains a BPE tokenizer on normalized lines with a small vocabulary (~3k) and defines special tokens including BOS/EOS. This matches the requirement to build a domain-appropriate tokenizer. The post-processor ensures BOS/EOS are applied consistently for single or paired sequences. The trained tokenizer is a prerequisite for packing and model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "{\n",
      "  \"vocab_size\": 3000,\n",
      "  \"bos_id\": 2,\n",
      "  \"eos_id\": 3,\n",
      "  \"pad_id\": 1,\n",
      "  \"unk_id\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "SPECIAL_TOKENS = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "VOCAB_SIZE = 3000\n",
    "\n",
    "# Train on normalized lines as corpus\n",
    "trainer = BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=SPECIAL_TOKENS)\n",
    "\n",
    "model = BPE(unk_token=\"<unk>\")\n",
    "tokenizer = Tokenizer(model)\n",
    "setattr(tokenizer, \"pre_tokenizer\", ByteLevel(add_prefix_space=True))\n",
    "setattr(tokenizer, \"decoder\", ByteLevelDecoder())\n",
    "\n",
    "tokenizer.train_from_iterator(norm_lines, trainer=trainer)\n",
    "\n",
    "# Add BOS/EOS processing\n",
    "setattr(\n",
    "    tokenizer,\n",
    "    \"post_processor\",\n",
    "    TemplateProcessing(\n",
    "        single=\"<bos> $A <eos>\",\n",
    "        pair=\"<bos> $A <eos> <bos> $B <eos>\",\n",
    "        special_tokens=[\n",
    "            (\"<bos>\", tokenizer.token_to_id(\"<bos>\")),\n",
    "            (\"<eos>\", tokenizer.token_to_id(\"<eos>\")),\n",
    "        ],\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"vocab_size\": tokenizer.get_vocab_size(),\n",
    "            \"bos_id\": tokenizer.token_to_id(\"<bos>\"),\n",
    "            \"eos_id\": tokenizer.token_to_id(\"<eos>\"),\n",
    "            \"pad_id\": tokenizer.token_to_id(\"<pad>\"),\n",
    "            \"unk_id\": tokenizer.token_to_id(\"<unk>\"),\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the tokenizer to disk and reloads it to verify vocabulary integrity and special token IDs. This ensures artifacts are consistent and ready for reuse across steps. Verifying IDs prevents silent mismatches in subsequent packing and model configuration. Persistence is necessary for reproducible training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'saved': True, 'path': 'tokenizer.json'}\n",
      "{\n",
      "  \"vocab_size\": 3000,\n",
      "  \"bos_id\": 2,\n",
      "  \"eos_id\": 3,\n",
      "  \"pad_id\": 1,\n",
      "  \"unk_id\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER_PATH = Path(\"tokenizer.json\")\n",
    "\n",
    "tokenizer.save(str(TOKENIZER_PATH))\n",
    "print({\"saved\": TOKENIZER_PATH.exists(), \"path\": str(TOKENIZER_PATH)})\n",
    "\n",
    "reloaded = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"vocab_size\": reloaded.get_vocab_size(),\n",
    "            \"bos_id\": reloaded.token_to_id(\"<bos>\"),\n",
    "            \"eos_id\": reloaded.token_to_id(\"<eos>\"),\n",
    "            \"pad_id\": reloaded.token_to_id(\"<pad>\"),\n",
    "            \"unk_id\": reloaded.token_to_id(\"<unk>\"),\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validates the trained tokenizer by round‑trip encode→decode on a sample line to ensure the vocabulary and BOS/EOS post‑processing behave correctly before packing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts special token IDs (PAD/BOS/EOS/UNK) from the persisted tokenizer for consistent configuration of the model, collator, and generation routines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"sample_preview\": \"Посвящается Любови Евгеньевне Белозерской\",\n",
      "  \"num_ids\": 17,\n",
      "  \"decoded_equals\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Round-trip encode/decode verification using the reloaded tokenizer\n",
    "# Bypass BOS/EOS in this check and compare after collapsing whitespace (incl. NBSP)\n",
    "sample = next((s for s in norm_lines if s.strip()), \"Sample text.\")\n",
    "enc = reloaded.encode(sample, add_special_tokens=False)\n",
    "dec = reloaded.decode(enc.ids, skip_special_tokens=True)\n",
    "\n",
    "sample_cmp = re.sub(r\"\\s+\", \" \", sample.replace(\"\\u00a0\", \" \")).strip()\n",
    "dec_cmp = re.sub(r\"\\s+\", \" \", dec.replace(\"\\u00a0\", \" \")).strip()\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"sample_preview\": sample_cmp[:80],\n",
    "            \"num_ids\": len(enc.ids),\n",
    "            \"decoded_equals\": dec_cmp == sample_cmp,\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")\n",
    "\n",
    "assert dec_cmp == sample_cmp, \"Round-trip encode/decode mismatch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"PAD_ID\": 1,\n",
      "  \"BOS_ID\": 2,\n",
      "  \"EOS_ID\": 3,\n",
      "  \"UNK_ID\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "PAD_ID = reloaded.token_to_id(\"<pad>\")\n",
    "BOS_ID = reloaded.token_to_id(\"<bos>\")\n",
    "EOS_ID = reloaded.token_to_id(\"<eos>\")\n",
    "UNK_ID = reloaded.token_to_id(\"<unk>\")\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"PAD_ID\": PAD_ID,\n",
    "            \"BOS_ID\": BOS_ID,\n",
    "            \"EOS_ID\": EOS_ID,\n",
    "            \"UNK_ID\": UNK_ID,\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packs token IDs to 512 tokens and builds a Dataset with input_ids and attention_mask. This operationalizes the assignment’s requirement to prepare tokenized data for pretraining; BOS/EOS are applied by the post-processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encode & buffer: 100%|██████████| 7383/7383 [00:37<00:00, 194.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"num_sequences\": 31736,\n",
      "  \"avg_len\": 512.0,\n",
      "  \"max_len\": 512\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Pack to fixed 512 with BOS/EOS per chunk (no cross-chunk slicing)\n",
    "MAX_LEN = 512\n",
    "buf = []\n",
    "input_ids = []\n",
    "attention_mask = []\n",
    "\n",
    "for t in tqdm(joined_text, desc=\"encode & buffer\"):\n",
    "    e = reloaded.encode(t, add_special_tokens=False).ids\n",
    "    buf.extend(e)\n",
    "    while len(buf) >= (MAX_LEN - 2):\n",
    "        chunk = [BOS_ID] + buf[: MAX_LEN - 2] + [EOS_ID]\n",
    "        input_ids.append(chunk)\n",
    "        attention_mask.append([1] * len(chunk))\n",
    "        buf = buf[MAX_LEN - 2 :]\n",
    "\n",
    "if buf:\n",
    "    chunk = [BOS_ID] + buf + [EOS_ID]\n",
    "    input_ids.append(chunk)\n",
    "    attention_mask.append([1] * len(chunk))\n",
    "\n",
    "train_ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"num_sequences\": len(train_ds),\n",
    "            \"avg_len\": round(\n",
    "                sum(len(x) for x in input_ids) / max(1, len(input_ids)),\n",
    "                1,\n",
    "            ),\n",
    "            \"max_len\": max((len(x) for x in input_ids), default=0),\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs a deterministic 95/5 train/validation split to enable evaluation during training. Supports Trainer’s evaluation strategy by providing a held-out split and helps monitor generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"train\": 30149,\n",
      "  \"validation\": 1587\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Deterministic 95/5 split by index\n",
    "split_idx = int(0.95 * len(train_ds))\n",
    "train_split = train_ds.select(range(0, split_idx))\n",
    "val_split = train_ds.select(range(split_idx, len(train_ds)))\n",
    "\n",
    "datasets_dict = DatasetDict(\n",
    "    {\n",
    "        \"train\": train_split,\n",
    "        \"validation\": val_split,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        {k: len(v) for k, v in datasets_dict.items()},\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wraps the saved tokenizer as a fast tokenizer compatible with Trainer and configures a causal LM data collator. Enables efficient batching and correct label shifting for next-token prediction; sanity-checks a tiny batch to catch shape issues early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"input_ids\": [\n",
      "    2,\n",
      "    512\n",
      "  ],\n",
      "  \"attention_mask\": [\n",
      "    2,\n",
      "    512\n",
      "  ],\n",
      "  \"labels\": [\n",
      "    2,\n",
      "    512\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Build a HF fast tokenizer from tokenizer.json for Trainer compatibility\n",
    "tok_fast = PreTrainedTokenizerFast(tokenizer_file=str(TOKENIZER_PATH))\n",
    "tok_fast.pad_token = \"<pad>\"\n",
    "tok_fast.unk_token = \"<unk>\"\n",
    "tok_fast.bos_token = \"<bos>\"\n",
    "tok_fast.eos_token = \"<eos>\"\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tok_fast,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Quick sanity: collate a tiny batch\n",
    "batch = data_collator([train_ds[0], train_ds[min(1, len(train_ds) - 1)]])\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            k: (getattr(v, \"shape\", None) or str(type(v)))\n",
    "            for k, v in batch.items()\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines a ~150M parameter Llama configuration with the assignment’s suggested sizes and sets special token IDs. Instantiates a decoder-only model aligned with the specified architecture; ties embeddings and checks for correct head/padding configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"num_params\": 128934912,\n",
      "  \"has_output_head\": true,\n",
      "  \"pad_token_id\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = reloaded.get_vocab_size()\n",
    "\n",
    "config = LlamaConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=1536,\n",
    "    num_hidden_layers=16,\n",
    "    num_attention_heads=16,\n",
    "    num_key_value_heads=8,\n",
    "    max_position_embeddings=512,\n",
    "    pad_token_id=PAD_ID,\n",
    "    bos_token_id=BOS_ID,\n",
    "    eos_token_id=EOS_ID,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM(config)\n",
    "# Ensure embeddings/head are tied and padding idx set\n",
    "model.resize_token_embeddings(VOCAB_SIZE)\n",
    "model.get_input_embeddings().padding_idx = PAD_ID\n",
    "model.tie_weights()\n",
    "\n",
    "# Quick checks\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "has_output_head = model.get_output_embeddings() is not None\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"num_params\": num_params,\n",
    "            \"has_output_head\": has_output_head,\n",
    "            \"pad_token_id\": model.config.pad_token_id,\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configures TrainingArguments and Trainer (effective batch size via accumulation, weight decay, warmup, fp16). Establishes evaluation/save strategies by epoch and creates a reproducible minimal training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer ready\n"
     ]
    }
   ],
   "source": [
    "# Effective batch size target 64–128 via gradient accumulation\n",
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = (\n",
    "    32  # adjust based on VRAM to hit effective batch size\n",
    ")\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"outputs/pretrain\",\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    num_train_epochs=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=datasets_dict[\"train\"],\n",
    "    eval_dataset=datasets_dict[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"trainer ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declares the 10 required evaluation prompts and registers an epoch-end callback to capture deterministic generations. Provides qualitative checks each epoch to complement validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-end generation callback registered\n"
     ]
    }
   ],
   "source": [
    "# 10 fixed pretrain prompts from assignment\n",
    "test_prompts = [\n",
    "    \"Все мысли, которые имеют огромные последствия\",\n",
    "    \"Сила войска зависит от его духа\",\n",
    "    \"Мысль о том, что он принес страдания\",\n",
    "    \"Человек сознает себя свободным\",\n",
    "    \"Что бы ни случилось, я всегда буду\",\n",
    "    \"Любовь мешает смерти\",\n",
    "    \"Нет, жизнь не кончена\",\n",
    "    \"Всякая мысль, даже самая простая\",\n",
    "    \"Война не любезность, а самое гадкое дело\",\n",
    "    \"Чтобы жить честно\",\n",
    "]\n",
    "\n",
    "# Minimal epoch-end generation callback\n",
    "\n",
    "\n",
    "class EpochGenCB(TrainerCallback):\n",
    "    def on_epoch_end(\n",
    "        self,\n",
    "        args,\n",
    "        state,\n",
    "        control,\n",
    "        model=None,\n",
    "        tokenizer=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        gens = []\n",
    "        gen_kwargs = dict(\n",
    "            max_new_tokens=64,\n",
    "            do_sample=False,\n",
    "            pad_token_id=PAD_ID,\n",
    "            eos_token_id=EOS_ID,\n",
    "        )\n",
    "        for p in test_prompts:\n",
    "            x = tok_fast(\n",
    "                [p],\n",
    "                return_tensors=\"pt\",\n",
    "                return_token_type_ids=False,\n",
    "            ).to(model.device)\n",
    "            with torch.inference_mode():\n",
    "                y = model.generate(**x, **gen_kwargs)\n",
    "            input_len = int(x[\"input_ids\"].shape[1])\n",
    "            cont_ids = y[0, input_len:]\n",
    "            cont_text = tok_fast.decode(\n",
    "                cont_ids,\n",
    "                skip_special_tokens=True,\n",
    "            )\n",
    "            gens.append({\"prompt\": p, \"text\": cont_text})\n",
    "        epoch_generations.append(\n",
    "            {\n",
    "                \"epoch\": int(state.epoch),\n",
    "                \"generations\": gens,\n",
    "            }\n",
    "        )\n",
    "        print(f\"Stored generations for epoch {int(state.epoch)}\")\n",
    "\n",
    "\n",
    "epoch_generations: list[dict] = []\n",
    "trainer.add_callback(EpochGenCB())\n",
    "print(\"epoch-end generation callback registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs training then evaluates to obtain validation loss and perplexity. This is the quantitative checkpoint for the language modeling objective and confirms end-to-end wiring of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='472' max='472' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [472/472 22:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.522200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.536700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.555200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.957300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.800100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.697400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.620300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored generations for epoch 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='794' max='794' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [794/794 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"train_metrics\": {\n",
      "    \"train_runtime\": 1354.0618,\n",
      "    \"train_samples_per_second\": 22.266,\n",
      "    \"train_steps_per_second\": 0.349,\n",
      "    \"total_flos\": 1.1657136948903936e+16,\n",
      "    \"train_loss\": 4.500001115314031,\n",
      "    \"epoch\": 1.0\n",
      "  },\n",
      "  \"eval_metrics\": {\n",
      "    \"eval_loss\": 3.8177576065063477,\n",
      "    \"eval_runtime\": 18.1985,\n",
      "    \"eval_samples_per_second\": 87.205,\n",
      "    \"eval_steps_per_second\": 43.63,\n",
      "    \"epoch\": 1.0\n",
      "  },\n",
      "  \"val_ppl\": 45.50206031164532\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "metrics_train = train_result.metrics\n",
    "\n",
    "# Evaluate to get validation loss\n",
    "metrics_eval = trainer.evaluate()\n",
    "val_loss = metrics_eval.get(\"eval_loss\", None)\n",
    "val_ppl = exp(val_loss) if val_loss is not None else None\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"train_metrics\": {\n",
    "                k: float(v)\n",
    "                for k, v in metrics_train.items()\n",
    "                if isinstance(v, (int, float))\n",
    "            },\n",
    "            \"eval_metrics\": {\n",
    "                k: float(v)\n",
    "                for k, v in metrics_eval.items()\n",
    "                if isinstance(v, (int, float))\n",
    "            },\n",
    "            \"val_ppl\": (float(val_ppl) if val_ppl is not None else None),\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the trained model and trainer state to disk for reproducibility and inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"saved_dir\": \"outputs/pretrain/final\",\n",
      "  \"exists\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = Path(\"outputs/pretrain/final\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "trainer.save_model(str(SAVE_DIR))  # saves model + tokenizer (tok_fast)\n",
    "trainer.state.save_to_json(str(SAVE_DIR / \"trainer_state.json\"))\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"saved_dir\": str(SAVE_DIR),\n",
    "            \"exists\": SAVE_DIR.exists(),\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the Alpaca RU parquet shard into a Dataset and prints split/row counts and columns to verify availability and schema prior to SFT formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'instruction', 'output'],\n",
      "        num_rows: 51760\n",
      "    })\n",
      "})\n",
      "{\n",
      "  \"num_rows\": 51760,\n",
      "  \"columns\": [\n",
      "    \"input\",\n",
      "    \"instruction\",\n",
      "    \"output\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load all local parquet shards\n",
    "parquet_paths = sorted(ALPACA_DIR.glob(\"*.parquet\"))\n",
    "alpaca = load_dataset(\"parquet\", data_files=[str(p) for p in parquet_paths])\n",
    "print(alpaca)\n",
    "train_split = alpaca[\"train\"]\n",
    "num_rows = int(getattr(train_split, \"num_rows\", 0))\n",
    "columns = list(getattr(train_split, \"column_names\", [])) or list(\n",
    "    getattr(getattr(train_split, \"features\", {}), \"keys\", lambda: [])()\n",
    ")\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"num_rows\": num_rows,\n",
    "            \"columns\": columns,\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maps Alpaca columns to dialog roles: `input→system`, `instruction→user`, `output→assistant`. Validates required columns and returns a clean dialog Dataset without pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"num_rows\": 51760,\n",
      "  \"columns\": [\n",
      "    \"system\",\n",
      "    \"user\",\n",
      "    \"assistant\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Map fields strictly: input→system, instruction→user, output→assistant (no pandas)\n",
    "sft_train = alpaca[\"train\"]\n",
    "feat = getattr(sft_train, \"features\", None)\n",
    "columns = (\n",
    "    list(feat.keys())\n",
    "    if feat is not None\n",
    "    else list(getattr(sft_train, \"column_names\", []))\n",
    ")\n",
    "required = {\"input\", \"instruction\", \"output\"}\n",
    "assert required.issubset(\n",
    "    set(columns)\n",
    "), f\"Missing required columns: {required - set(columns)}\"\n",
    "\n",
    "\n",
    "def _map(row):\n",
    "    return {\n",
    "        \"system\": (row.get(\"input\") or \"\"),\n",
    "        \"user\": (row.get(\"instruction\") or \"\"),\n",
    "        \"assistant\": (row.get(\"output\") or \"\"),\n",
    "    }\n",
    "\n",
    "\n",
    "sft_mapped = sft_train.map(_map, remove_columns=sft_train.column_names)\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"num_rows\": int(getattr(sft_mapped, \"num_rows\", 0)),\n",
    "            \"columns\": list(getattr(sft_mapped, \"features\", {}).keys()),\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializes the Qwen2.5‑0.5B base model and tokenizer for SFT. Chooses float16 on CUDA, ensures `pad_token`/`eos_token` are set for batching and deterministic generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"pad_token\": \"<|endoftext|>\",\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"eos_token\": \"<|endoftext|>\",\n",
      "  \"eos_token_id\": 151643\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "QWEN_ID = \"Qwen/Qwen2.5-0.5B\"\n",
    "qwen_tok = AutoTokenizer.from_pretrained(\n",
    "    QWEN_ID,\n",
    "    local_files_only=False,\n",
    ")\n",
    "\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    QWEN_ID,\n",
    "    dtype=(torch.bfloat16 if torch.cuda.is_available() else torch.float32),\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "# Ensure pad and eos are set\n",
    "if qwen_tok.pad_token is None:\n",
    "    qwen_tok.pad_token = qwen_tok.eos_token or \"<|endoftext|>\"\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"pad_token\": qwen_tok.pad_token,\n",
    "            \"pad_token_id\": qwen_tok.pad_token_id,\n",
    "            \"eos_token\": qwen_tok.eos_token,\n",
    "            \"eos_token_id\": qwen_tok.eos_token_id,\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Builds dialog texts via the tokenizer’s chat template, creates a small validation split, defines SFT `TrainingArguments`, and instantiates `SFTTrainer` on the Alpaca RU dialog Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build sft texts:   0%|          | 0/51760 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build sft texts: 100%|██████████| 51760/51760 [00:05<00:00, 9544.99it/s] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd48ee444ae4edea5f3113e7fd906b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/49172 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa6f8e20b664eb78e88aed0ce0069b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/49172 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5381d3f3af439bb8b9d1450a21b735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/49172 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7063af6304514d08b299251cc2b33155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/2588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1329f36fdf0e46448a41a29d0b6e5ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/2588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f07529bf38d46ad9d57b60ba98d7642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/2588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sft trainer ready (completion-only)\n"
     ]
    }
   ],
   "source": [
    "# Build conversational texts via tokenizer chat template (no pandas)\n",
    "messages_texts = []\n",
    "for r in tqdm(sft_mapped, desc=\"build sft texts\"):\n",
    "    messages = []\n",
    "    sys = (r.get(\"system\") or \"\").strip()\n",
    "    if sys:\n",
    "        messages.append({\"role\": \"system\", \"content\": sys})\n",
    "    messages.append({\"role\": \"user\", \"content\": r.get(\"user\") or \"\"})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": r.get(\"assistant\") or \"\"})\n",
    "    text = qwen_tok.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "    messages_texts.append(text)\n",
    "\n",
    "sft_text_ds = Dataset.from_dict({\"text\": messages_texts})\n",
    "# Small validation split\n",
    "sft_splits = sft_text_ds.train_test_split(test_size=0.05, seed=SEED)\n",
    "\n",
    "sft_args = TrainingArguments(\n",
    "    output_dir=\"outputs/sft\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=32,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.0,\n",
    "    warmup_ratio=0.03,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=50,\n",
    "    bf16=torch.cuda.is_available(),\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=qwen_model,\n",
    "    processing_class=qwen_tok,\n",
    "    args=sft_args,\n",
    "    train_dataset=sft_splits[\"train\"],\n",
    "    eval_dataset=sft_splits[\"test\"],\n",
    ")\n",
    "\n",
    "print(\"sft trainer ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs the SFT training loop and prints core metrics. Confirms that data formatting, optimization, and device settings are correctly integrated; produces an instruction‑tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='769' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 56/769 03:45 < 49:39, 0.24 it/s, Epoch 0.07/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m sft_train_result = \u001b[43msft_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m      3\u001b[39m     json.dumps(\n\u001b[32m      4\u001b[39m         {\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     )\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/yandex_dl_course_6_private/.venv/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/yandex_dl_course_6_private/.venv/lib/python3.12/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/yandex_dl_course_6_private/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:1178\u001b[39m, in \u001b[36mSFTTrainer.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1177\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maybe_activation_offload_context:\n\u001b[32m-> \u001b[39m\u001b[32m1178\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/yandex_dl_course_6_private/.venv/lib/python3.12/site-packages/transformers/trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/yandex_dl_course_6_private/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:1116\u001b[39m, in \u001b[36mSFTTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   1114\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1115\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mExpected \u001b[39m\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mposition_ids\u001b[39m\u001b[33m'\u001b[39m\u001b[33m in inputs.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1116\u001b[39m         entropy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgather_for_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentropy\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1117\u001b[39m     \u001b[38;5;28mself\u001b[39m._metrics[mode][\u001b[33m\"\u001b[39m\u001b[33mentropy\u001b[39m\u001b[33m\"\u001b[39m].append(entropy)\n\u001b[32m   1119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1120\u001b[39m     \u001b[38;5;66;03m# When using padding-free, the attention_mask is not present in the inputs, instead we have cu_seq_lens_q,\u001b[39;00m\n\u001b[32m   1121\u001b[39m     \u001b[38;5;66;03m# cu_seq_lens_k, and max_length_k, max_length_q and position_ids.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "sft_train_result = sft_trainer.train()\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            k: float(v)\n",
    "            for k, v in sft_train_result.metrics.items()\n",
    "            if isinstance(v, (int, float))\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates answers for four evaluation prompts to qualitatively assess instruction‑following. Uses deterministic generation for comparability and formats prompts via the same chat template used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answers for the 4 evaluation questions\n",
    "questions_rus = [\n",
    "    \"сколько планет в нашей солнечной системе?\",\n",
    "    \"расскажи стих\",\n",
    "    \"когда собирать крыжовник?\",\n",
    "    \"Как быстро выучить новый язык?\",\n",
    "]\n",
    "\n",
    "responses = []\n",
    "for q in questions_rus:\n",
    "    msgs = []\n",
    "    system_text = \"\"\n",
    "    if system_text:\n",
    "        msgs.append({\"role\": \"system\", \"content\": system_text})\n",
    "    msgs.append({\"role\": \"user\", \"content\": q})\n",
    "    prompt = qwen_tok.apply_chat_template(\n",
    "        msgs,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    inputs = qwen_tok(\n",
    "        [prompt],\n",
    "        return_tensors=\"pt\",\n",
    "        return_token_type_ids=False,\n",
    "    ).to(qwen_model.device)\n",
    "    with torch.inference_mode():\n",
    "        out = qwen_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            pad_token_id=qwen_tok.pad_token_id,\n",
    "            eos_token_id=qwen_tok.eos_token_id,\n",
    "        )\n",
    "    # Slice out only the generated continuation after the prompt\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    gen_ids = out[0, input_len:]\n",
    "    text = qwen_tok.decode(gen_ids, skip_special_tokens=True)\n",
    "    responses.append({\"question\": q, \"answer\": text})\n",
    "\n",
    "for r in responses:\n",
    "    print(\"Q:\", r[\"question\"])\n",
    "    print(\"A:\", r[\"answer\"])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the SFT‑tuned model and tokenizer to disk for reproducibility and reuse. Enables others to validate and deploy the fine‑tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SFT model and tokenizer\n",
    "\n",
    "SFT_DIR = Path(\"outputs/sft/final\")\n",
    "SFT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sft_trainer.save_model(str(SFT_DIR))\n",
    "qwen_tok.save_pretrained(str(SFT_DIR))\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\"sft_saved_dir\": str(SFT_DIR), \"exists\": SFT_DIR.exists()},\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reports environment and library versions, model parameter counts, and dataset sizes for reproducibility/debugging. Captures runtime context to prevent silent drift across environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"seed\": SEED,\n",
    "    \"device\": str(device),\n",
    "    \"device_name\": device_name,\n",
    "    \"dtype\": str(dtype),\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"torch\": torch.__version__,\n",
    "    \"transformers\": __import__(\"transformers\").__version__,\n",
    "    \"datasets\": __import__(\"datasets\").__version__,\n",
    "    \"trl\": __import__(\"trl\").__version__,\n",
    "    \"pretrain\": {\n",
    "        \"vocab_size\": int(VOCAB_SIZE),\n",
    "        \"num_params\": int(sum(p.numel() for p in model.parameters())),\n",
    "        \"context_len\": int(model.config.max_position_embeddings),\n",
    "    },\n",
    "    \"sft\": {\n",
    "        \"base_model\": QWEN_ID,\n",
    "        \"train_rows\": int(getattr(sft_splits[\"train\"], \"num_rows\", 0)),\n",
    "        \"eval_rows\": int(getattr(sft_splits[\"test\"], \"num_rows\", 0)),\n",
    "    },\n",
    "}\n",
    "print(json.dumps(summary, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consolidates the last epoch’s pretrain generations and the SFT evaluation answers. Satisfies the requirement to show generation outputs in the notebook for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated displays: 10 pretrain generations (last epoch) and 4 SFT answers\n",
    "\n",
    "if epoch_generations:\n",
    "    last = epoch_generations[-1]\n",
    "    print(f\"Pretrain epoch {last['epoch']} generations:\")\n",
    "    for i, g in enumerate(last[\"generations\"], 1):\n",
    "        print(f\"[{i:02d}] prompt: {g['prompt']}\")\n",
    "        print(f\"     text: {g['text']}\")\n",
    "        print(\"---\")\n",
    "else:\n",
    "    print(\"No pretrain generations recorded.\")\n",
    "\n",
    "print(\"SFT evaluation answers:\")\n",
    "for r in responses:\n",
    "    print(\"Q:\", r[\"question\"])\n",
    "    print(\"A:\", r[\"answer\"])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persists generations and evaluation outputs to JSON/CSV for external inspection and comparison. These machine‑readable artifacts mirror on‑screen results for analysis and archiving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist outputs to JSON/CSV\n",
    "\n",
    "OUT_DIR = Path(\"outputs/reports\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Pretrain generations per epoch to JSON\n",
    "with (OUT_DIR / \"pretrain_epoch_generations.json\").open(\n",
    "    \"w\", encoding=\"utf-8\"\n",
    ") as f:\n",
    "    json.dump(epoch_generations, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Last epoch generations to CSV\n",
    "with (OUT_DIR / \"pretrain_last_epoch_generations.csv\").open(\n",
    "    \"w\", encoding=\"utf-8\", newline=\"\"\n",
    ") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"epoch\", \"idx\", \"prompt\", \"text\"])\n",
    "    if epoch_generations:\n",
    "        last = epoch_generations[-1]\n",
    "        for i, g in enumerate(last[\"generations\"], 1):\n",
    "            w.writerow([last[\"epoch\"], i, g[\"prompt\"], g[\"text\"]])\n",
    "\n",
    "# SFT responses to CSV\n",
    "with (OUT_DIR / \"sft_eval_responses.csv\").open(\n",
    "    \"w\", encoding=\"utf-8\", newline=\"\"\n",
    ") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"question\", \"answer\"])\n",
    "    for r in responses:\n",
    "        w.writerow([r[\"question\"], r[\"answer\"]])\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\"written\": True, \"dir\": str(OUT_DIR)}, ensure_ascii=False, indent=2\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
