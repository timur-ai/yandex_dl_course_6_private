{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Pretraining and SFT (Single Notebook)\n",
    "\n",
    "This notebook implements all steps required by the PRD in a single place:\n",
    "\n",
    "- Pretrain on a Russian literature corpus (local `data/corpus/`) to learn language structure.\n",
    "- Train a custom BPE tokenizer (~3k vocab) with `<unk>`, `<pad>`, `<bos>`, `<eos>`.\n",
    "- Build tokenized datasets and train a ~150M decoder-only model (context 512) with HF Trainer.\n",
    "- After each epoch, run deterministic generations on the 10 assignment prompts and display them.\n",
    "- SFT Qwen2.5-0.5B using local `data/alpaca-cleaned-ru/` in conversational format (system/user/assistant).\n",
    "- Evaluate on 4 assignment questions and display outputs.\n",
    "\n",
    "Constraints:\n",
    "- One notebook only (no external scripts/configs).\n",
    "- Use `uv` for environment and `ruff` for linting.\n",
    "- Keep logs/docs in English; dataset strings and model generations remain Russian where appropriate.\n",
    "\n",
    "Run environment:\n",
    "- Start via: `uv run jupyter lab` (or `uv run jupyter notebook`).\n",
    "- Python packages and versions come from `pyproject.toml`/`uv.lock`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    set_seed,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    set_seed(seed)\n",
    "\n",
    "\n",
    "SEED = int(os.environ.get(\"SEED\", \"42\"))\n",
    "seed_everything(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_name = \"CPU\"\n",
    "    dtype = torch.float32\n",
    "\n",
    "print({\n",
    "    \"seed\": SEED,\n",
    "    \"device\": str(device),\n",
    "    \"device_name\": device_name,\n",
    "    \"cuda\": torch.cuda.is_available(),\n",
    "    \"dtype\": str(dtype),\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "\n",
    "CORPUS_DIR = Path(\"data/corpus\")\n",
    "ALPACA_DIR = Path(\"data/alpaca-cleaned-ru\")\n",
    "\n",
    "assert CORPUS_DIR.exists() and CORPUS_DIR.is_dir(), f\"Missing directory: {CORPUS_DIR}\"\n",
    "assert ALPACA_DIR.exists() and ALPACA_DIR.is_dir(), f\"Missing directory: {ALPACA_DIR}\"\n",
    "\n",
    "corpus_txt_files = sorted(p for p in CORPUS_DIR.glob(\"**/*.txt\"))\n",
    "parquet_files = sorted(p for p in ALPACA_DIR.glob(\"*.parquet\"))\n",
    "\n",
    "stats = {\n",
    "    \"corpus_num_files\": len(corpus_txt_files),\n",
    "    \"corpus_sample\": [str(p) for p in corpus_txt_files[:3]],\n",
    "    \"alpaca_parquet_files\": [str(p) for p in parquet_files],\n",
    "}\n",
    "\n",
    "print(json.dumps(stats, ensure_ascii=False, indent=2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
