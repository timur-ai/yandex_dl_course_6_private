{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Pretraining and SFT (Single Notebook)\n",
    "\n",
    "This notebook implements all steps required by the PRD in a single place:\n",
    "\n",
    "- Pretrain on a Russian literature corpus (local `data/corpus/`) to learn language structure.\n",
    "- Train a custom BPE tokenizer (~3k vocab) with `<unk>`, `<pad>`, `<bos>`, `<eos>`.\n",
    "- Build tokenized datasets and train a ~150M decoder-only model (context 512) with HF Trainer.\n",
    "- After each epoch, run deterministic generations on the 10 assignment prompts and display them.\n",
    "- SFT Qwen2.5-0.5B using local `data/alpaca-cleaned-ru/` in conversational format (system/user/assistant).\n",
    "- Evaluate on 4 assignment questions and display outputs.\n",
    "\n",
    "Constraints:\n",
    "- One notebook only (no external scripts/configs).\n",
    "- Use `uv` for environment and `ruff` for linting.\n",
    "- Keep logs/docs in English; dataset strings and model generations remain Russian where appropriate.\n",
    "\n",
    "Run environment:\n",
    "- Start via: `uv run jupyter lab` (or `uv run jupyter notebook`).\n",
    "- Python packages and versions come from `pyproject.toml`/`uv.lock`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    set_seed,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SEED = int(os.environ.get(\"SEED\", \"42\"))\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "set_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_name = \"CPU\"\n",
    "    dtype = torch.float32\n",
    "\n",
    "print({\n",
    "    \"seed\": SEED,\n",
    "    \"device\": str(device),\n",
    "    \"device_name\": device_name,\n",
    "    \"cuda\": torch.cuda.is_available(),\n",
    "    \"dtype\": str(dtype),\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "\n",
    "CORPUS_DIR = Path(\"data/corpus\")\n",
    "ALPACA_DIR = Path(\"data/alpaca-cleaned-ru\")\n",
    "\n",
    "assert CORPUS_DIR.exists() and CORPUS_DIR.is_dir(), f\"Missing directory: {CORPUS_DIR}\"\n",
    "assert ALPACA_DIR.exists() and ALPACA_DIR.is_dir(), f\"Missing directory: {ALPACA_DIR}\"\n",
    "\n",
    "corpus_txt_files = sorted(p for p in CORPUS_DIR.glob(\"**/*.txt\"))\n",
    "parquet_files = sorted(p for p in ALPACA_DIR.glob(\"*.parquet\"))\n",
    "\n",
    "stats = {\n",
    "    \"corpus_num_files\": len(corpus_txt_files),\n",
    "    \"corpus_sample\": [str(p) for p in corpus_txt_files[:3]],\n",
    "    \"alpaca_parquet_files\": [str(p) for p in parquet_files],\n",
    "}\n",
    "\n",
    "print(json.dumps(stats, ensure_ascii=False, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CORPUS_DIR = Path(\"data/corpus\")\n",
    "files = sorted(CORPUS_DIR.glob(\"**/*.txt\"))\n",
    "\n",
    "num_files = len(files)\n",
    "num_lines = 0\n",
    "num_chars = 0\n",
    "\n",
    "for p in files:\n",
    "    with p.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            num_lines += 1\n",
    "            num_chars += len(line)\n",
    "\n",
    "avg_chars_per_line = (num_chars / num_lines) if num_lines else 0.0\n",
    "avg_lines_per_file = (num_lines / num_files) if num_files else 0.0\n",
    "\n",
    "print({\n",
    "    \"num_files\": num_files,\n",
    "    \"num_lines\": num_lines,\n",
    "    \"num_chars\": num_chars,\n",
    "    \"avg_chars_per_line\": round(avg_chars_per_line, 2),\n",
    "    \"avg_lines_per_file\": round(avg_lines_per_file, 2),\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "CORPUS_DIR = Path(\"data/corpus\")\n",
    "files = sorted(CORPUS_DIR.glob(\"**/*.txt\"))\n",
    "\n",
    "# Read all lines, strip trailing whitespace; group into paragraphs by blank lines\n",
    "all_lines: List[str] = []\n",
    "paragraphs: List[str] = []\n",
    "current_paragraph: List[str] = []\n",
    "\n",
    "for p in files:\n",
    "    with p.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.rstrip(\"\\n\\r\")\n",
    "            all_lines.append(line)\n",
    "            if line.strip():\n",
    "                current_paragraph.append(line)\n",
    "            else:\n",
    "                if current_paragraph:\n",
    "                    paragraphs.append(\"\\n\".join(current_paragraph))\n",
    "                    current_paragraph = []\n",
    "        if current_paragraph:\n",
    "            paragraphs.append(\"\\n\".join(current_paragraph))\n",
    "            current_paragraph = []\n",
    "\n",
    "num_lines_before = len(all_lines)\n",
    "num_paragraphs_before = len(paragraphs)\n",
    "\n",
    "# Deduplicate exact lines and exact paragraphs\n",
    "unique_lines = list(dict.fromkeys(all_lines))\n",
    "unique_paragraphs = list(dict.fromkeys(paragraphs))\n",
    "\n",
    "print({\n",
    "    \"lines_before\": num_lines_before,\n",
    "    \"lines_after_unique\": len(unique_lines),\n",
    "    \"paragraphs_before\": num_paragraphs_before,\n",
    "    \"paragraphs_after_unique\": len(unique_paragraphs),\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Allow punctuation, digits, whitespace, and Cyrillic letters (incl. Ё/ё). Filter out lines containing non-Cyrillic letters.\n",
    "ALLOWED_CHARS = set(\" \\t\\n\\r0123456789!?,.:;-—()[]{}'\\\"“…»«/\\\\|@#$%^&*+=~`<>·•–\\u00A0\\u2000\\u2001\\u2002\\u2003\\u2009\\u2026\")\n",
    "\n",
    "\n",
    "def is_cyrillic_line(line: str) -> bool:\n",
    "    for ch in line:\n",
    "        if ch.isalpha():\n",
    "            if not (\"А\" <= ch <= \"Я\" or \"а\" <= ch <= \"я\" or ch in (\"Ё\", \"ё\")):\n",
    "                return False\n",
    "        elif not (ch in ALLOWED_CHARS or ch.isspace()):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "filtered_lines: List[str] = [ln for ln in unique_lines if is_cyrillic_line(ln)]\n",
    "print({\n",
    "    \"lines_before\": len(unique_lines),\n",
    "    \"lines_after_cyrillic_filter\": len(filtered_lines),\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Start from filtered_lines; normalize repeated punctuation and whitespace.\n",
    "# Rules:\n",
    "# - Replace sequences of ! or ? with a single char.\n",
    "# - Normalize ellipsis variations to a single … (or three dots).\n",
    "# - Collapse multiple spaces/tabs to a single space (preserve newlines).\n",
    "# - Trim trailing spaces.\n",
    "\n",
    "norm_lines: List[str] = []\n",
    "for ln in filtered_lines:\n",
    "    s = ln\n",
    "    s = re.sub(r\"[!]{2,}\", \"!\", s)\n",
    "    s = re.sub(r\"[?]{2,}\", \"?\", s)\n",
    "    s = re.sub(r\"(\\.{3,}|…{2,})\", \"…\", s)\n",
    "    s = re.sub(r\"[ \\t]{2,}\", \" \", s)\n",
    "    s = s.rstrip()\n",
    "    norm_lines.append(s)\n",
    "\n",
    "print({\n",
    "    \"lines_before\": len(filtered_lines),\n",
    "    \"lines_after_norm\": len(norm_lines),\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming norm_lines exist; join into paragraphs and then into chunks under a char budget as proxy for 512 tokens.\n",
    "# Reserve room for BOS/EOS by targeting slightly below 512 tokens (approx. characters here). Adjust later after tokenizer.\n",
    "CHAR_BUDGET = 2000  # rough proxy; to be replaced by token-based chunking after tokenizer training\n",
    "\n",
    "joined_text = []\n",
    "for para in unique_paragraphs:\n",
    "    s = para.replace(\"\\r\", \"\").strip()\n",
    "    if s:\n",
    "        joined_text.append(s)\n",
    "\n",
    "chunks: List[str] = []\n",
    "current = []\n",
    "current_len = 0\n",
    "\n",
    "for para in joined_text:\n",
    "    # prefer to add a newline between paragraphs\n",
    "    block = (\"\\n\" if current else \"\") + para\n",
    "    if current_len + len(block) <= CHAR_BUDGET:\n",
    "        current.append(para)\n",
    "        current_len += len(block)\n",
    "    else:\n",
    "        if current:\n",
    "            chunks.append(\"\\n\".join(current))\n",
    "        # if a single paragraph is too long, hard-split into sub-blocks\n",
    "        if len(para) > CHAR_BUDGET:\n",
    "            start = 0\n",
    "            while start < len(para):\n",
    "                end = min(start + CHAR_BUDGET, len(para))\n",
    "                chunks.append(para[start:end])\n",
    "                start = end\n",
    "            current = []\n",
    "            current_len = 0\n",
    "        else:\n",
    "            current = [para]\n",
    "            current_len = len(para)\n",
    "\n",
    "if current:\n",
    "    chunks.append(\"\\n\".join(current))\n",
    "\n",
    "print({\n",
    "    \"num_chunks\": len(chunks),\n",
    "    \"avg_chunk_chars\": round(sum(len(c) for c in chunks) / max(1, len(chunks)), 1),\n",
    "    \"min_chunk_chars\": min((len(c) for c in chunks), default=0),\n",
    "    \"max_chunk_chars\": max((len(c) for c in chunks), default=0),\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "pre_stats = {\n",
    "    \"initial_files\": len(corpus_txt_files),\n",
    "    \"initial_lines\": num_lines_before,\n",
    "}\n",
    "\n",
    "post_stats = {\n",
    "    \"unique_lines\": len(unique_lines),\n",
    "    \"unique_paragraphs\": len(unique_paragraphs),\n",
    "    \"cyrillic_lines\": len(filtered_lines),\n",
    "    \"normalized_lines\": len(norm_lines),\n",
    "    \"chunks\": len(chunks),\n",
    "    \"avg_chunk_chars\": round(sum(len(c) for c in chunks) / max(1, len(chunks)), 1),\n",
    "}\n",
    "\n",
    "print(\"Preprocessing summary:\\n\" + json.dumps({\"pre\": pre_stats, \"post\": post_stats}, ensure_ascii=False, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "SPECIAL_TOKENS = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "VOCAB_SIZE = 3000\n",
    "\n",
    "# Train on normalized lines as corpus\n",
    "trainer = BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=SPECIAL_TOKENS)\n",
    "\n",
    "model = BPE(unk_token=\"<unk>\")\n",
    "tokenizer = Tokenizer(model)\n",
    "# Assign via setattr to avoid over-strict attribute checks\n",
    "setattr(tokenizer, \"pre_tokenizer\", Whitespace())\n",
    "\n",
    "tokenizer.train_from_iterator(norm_lines, trainer=trainer)\n",
    "\n",
    "# Add BOS/EOS processing\n",
    "setattr(tokenizer, \"post_processor\", TemplateProcessing(\n",
    "    single=\"<bos> $A <eos>\",\n",
    "    pair=\"<bos> $A <eos> <bos> $B <eos>\",\n",
    "    special_tokens=[\n",
    "        (\"<bos>\", tokenizer.token_to_id(\"<bos>\")),\n",
    "        (\"<eos>\", tokenizer.token_to_id(\"<eos>\")),\n",
    "    ],\n",
    "))\n",
    "\n",
    "print({\n",
    "    \"vocab_size\": tokenizer.get_vocab_size(),\n",
    "    \"bos_id\": tokenizer.token_to_id(\"<bos>\"),\n",
    "    \"eos_id\": tokenizer.token_to_id(\"<eos>\"),\n",
    "    \"pad_id\": tokenizer.token_to_id(\"<pad>\"),\n",
    "    \"unk_id\": tokenizer.token_to_id(\"<unk>\"),\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "TOKENIZER_PATH = Path(\"tokenizer.json\")\n",
    "\n",
    "tokenizer.save(str(TOKENIZER_PATH))\n",
    "print({\"saved\": TOKENIZER_PATH.exists(), \"path\": str(TOKENIZER_PATH)})\n",
    "\n",
    "reloaded = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
    "print({\n",
    "    \"vocab_size\": reloaded.get_vocab_size(),\n",
    "    \"bos_id\": reloaded.token_to_id(\"<bos>\"),\n",
    "    \"eos_id\": reloaded.token_to_id(\"<eos>\"),\n",
    "    \"pad_id\": reloaded.token_to_id(\"<pad>\"),\n",
    "    \"unk_id\": reloaded.token_to_id(\"<unk>\"),\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round-trip encode/decode verification using the reloaded tokenizer\n",
    "# Use a non-empty line from the corpus-derived normalized lines to avoid embedding literals\n",
    "sample = next((s for s in norm_lines if s.strip()), \"Sample text.\")\n",
    "enc = reloaded.encode(sample)\n",
    "dec = reloaded.decode(enc.ids)\n",
    "\n",
    "print({\n",
    "    \"sample_preview\": sample[:80],\n",
    "    \"num_ids\": len(enc.ids),\n",
    "    \"decoded_equals\": dec == sample,\n",
    "})\n",
    "\n",
    "assert dec == sample, \"Round-trip encode/decode mismatch\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = reloaded.token_to_id(\"<pad>\")\n",
    "BOS_ID = reloaded.token_to_id(\"<bos>\")\n",
    "EOS_ID = reloaded.token_to_id(\"<eos>\")\n",
    "UNK_ID = reloaded.token_to_id(\"<unk>\")\n",
    "\n",
    "print({\n",
    "    \"PAD_ID\": PAD_ID,\n",
    "    \"BOS_ID\": BOS_ID,\n",
    "    \"EOS_ID\": EOS_ID,\n",
    "    \"UNK_ID\": UNK_ID,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Tokenize chunks to input_ids and attention_mask\n",
    "encoded = [reloaded.encode(t) for t in chunks]\n",
    "input_ids = [e.ids for e in encoded]\n",
    "attention_mask = [[1] * len(e.ids) for e in encoded]\n",
    "\n",
    "train_ds = Dataset.from_dict({\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\": attention_mask,\n",
    "})\n",
    "\n",
    "print(train_ds)\n",
    "print(train_ds[0].keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Deterministic 95/5 split by index\n",
    "split_idx = int(0.95 * len(train_ds))\n",
    "train_split = train_ds.select(range(0, split_idx))\n",
    "val_split = train_ds.select(range(split_idx, len(train_ds)))\n",
    "\n",
    "datasets_dict = DatasetDict({\n",
    "    \"train\": train_split,\n",
    "    \"validation\": val_split,\n",
    "})\n",
    "\n",
    "print(datasets_dict)\n",
    "for k in datasets_dict:\n",
    "    print(k, len(datasets_dict[k]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, DataCollatorForLanguageModeling\n",
    "\n",
    "# Build a HF fast tokenizer from tokenizer.json for Trainer compatibility\n",
    "tok_fast = PreTrainedTokenizerFast(tokenizer_file=str(TOKENIZER_PATH))\n",
    "tok_fast.pad_token = \"<pad>\"\n",
    "tok_fast.unk_token = \"<unk>\"\n",
    "tok_fast.bos_token = \"<bos>\"\n",
    "tok_fast.eos_token = \"<eos>\"\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tok_fast,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Quick sanity: collate a tiny batch\n",
    "batch = data_collator([train_ds[0], train_ds[min(1, len(train_ds)-1)]])\n",
    "print({k: (v.shape if hasattr(v, 'shape') else type(v)) for k, v in batch.items()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "VOCAB_SIZE = reloaded.get_vocab_size()\n",
    "\n",
    "config = LlamaConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=1536,\n",
    "    num_hidden_layers=16,\n",
    "    num_attention_heads=16,\n",
    "    num_key_value_heads=8,\n",
    "    max_position_embeddings=512,\n",
    "    pad_token_id=PAD_ID,\n",
    "    bos_token_id=BOS_ID,\n",
    "    eos_token_id=EOS_ID,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM(config)\n",
    "# Ensure embeddings/head are tied and padding idx set\n",
    "model.resize_token_embeddings(VOCAB_SIZE)\n",
    "model.get_input_embeddings().padding_idx = PAD_ID\n",
    "model.tie_weights()\n",
    "\n",
    "# Quick checks\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "has_output_head = model.get_output_embeddings() is not None\n",
    "print({\n",
    "    \"num_params\": num_params,\n",
    "    \"has_output_head\": has_output_head,\n",
    "    \"pad_token_id\": model.config.pad_token_id,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Effective batch size target 64–128 via gradient accumulation\n",
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = 32  # adjust based on VRAM to hit effective batch size\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"outputs/pretrain\",\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    num_train_epochs=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    bf16=False,\n",
    "    logging_steps=50,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=datasets_dict[\"train\"],\n",
    "    eval_dataset=datasets_dict[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"trainer ready\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria\n",
    "\n",
    "class EosStopping(StoppingCriteria):\n",
    "    def __init__(self, eos_token_id: int) -> None:\n",
    "        self.eos = eos_token_id\n",
    "    def __call__(self, input_ids, scores) -> bool:  # type: ignore[override]\n",
    "        return (input_ids[0, -1].item() == self.eos)\n",
    "\n",
    "# Placeholder list for 10 assignment prompts (loaded externally at grading time)\n",
    "def generate_on_prompts(model, tok: PreTrainedTokenizerFast, prompts: list[str]) -> list[dict]:\n",
    "    outputs = []\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=64,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        top_p=1.0,\n",
    "        num_beams=1,\n",
    "        pad_token_id=PAD_ID,\n",
    "        eos_token_id=EOS_ID,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    for p in prompts:\n",
    "        inputs = tok([p], return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        with torch.inference_mode():\n",
    "            out_ids = model.generate(**inputs, **gen_kwargs)\n",
    "        text = tok.batch_decode(out_ids, skip_special_tokens=True)[0]\n",
    "        outputs.append({\"prompt\": p, \"text\": text})\n",
    "    return outputs\n",
    "\n",
    "# Hook at epoch end\n",
    "epoch_generations: list[dict] = []\n",
    "\n",
    "orig_callback = trainer.callback_handler\n",
    "\n",
    "def on_epoch_end(tr, args, state, control):\n",
    "    # prompts must be provided externally in a cell before training\n",
    "    if \"test_prompts\" in globals() and isinstance(globals()[\"test_prompts\"], list):\n",
    "        gens = generate_on_prompts(tr.model, tok_fast, globals()[\"test_prompts\"]) \n",
    "        epoch_generations.append({\"epoch\": int(state.epoch), \"generations\": gens})\n",
    "        print(f\"Stored generations for epoch {int(state.epoch)}\")\n",
    "\n",
    "trainer.add_callback(type(\"EpochGenCB\", (), {\"on_epoch_end\": staticmethod(on_epoch_end)}))\n",
    "print(\"epoch-end generation callback registered\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "train_result = trainer.train()\n",
    "metrics_train = train_result.metrics\n",
    "\n",
    "# Evaluate to get validation loss\n",
    "metrics_eval = trainer.evaluate()\n",
    "val_loss = metrics_eval.get(\"eval_loss\", None)\n",
    "val_ppl = (exp(val_loss) if val_loss is not None else None)\n",
    "\n",
    "print({\n",
    "    \"train_metrics\": {k: float(v) for k, v in metrics_train.items() if isinstance(v, (int, float))},\n",
    "    \"eval_metrics\": {k: float(v) for k, v in metrics_eval.items() if isinstance(v, (int, float))},\n",
    "    \"val_ppl\": (float(val_ppl) if val_ppl is not None else None),\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "SAVE_DIR = Path(\"outputs/pretrain/final\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "trainer.save_model(str(SAVE_DIR))  # saves model + tokenizer (tok_fast)\n",
    "trainer.state.save_to_json(str(SAVE_DIR / \"trainer_state.json\"))\n",
    "\n",
    "print({\n",
    "    \"saved_dir\": str(SAVE_DIR),\n",
    "    \"exists\": SAVE_DIR.exists(),\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "alpaca = load_dataset(\"parquet\", data_files=str(ALPACA_DIR / \"train-00000-of-00001-*.parquet\"))\n",
    "print(alpaca)\n",
    "train_split = alpaca[\"train\"]\n",
    "num_rows = int(getattr(train_split, \"num_rows\", 0))\n",
    "columns = list(getattr(getattr(train_split, \"features\", {}), \"keys\", lambda: [])())\n",
    "print({\n",
    "    \"num_rows\": num_rows,\n",
    "    \"columns\": columns,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map fields strictly: input→system, instruction→user, output→assistant\n",
    "# Ensure non-optional system with empty string default\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "sft_train = alpaca[\"train\"]\n",
    "feat = getattr(sft_train, \"features\", None)\n",
    "columns = list(feat.keys()) if feat is not None else list(getattr(sft_train, \"column_names\", []))\n",
    "required = {\"input\", \"instruction\", \"output\"}\n",
    "assert required.issubset(set(columns)), f\"Missing required columns: {required - set(columns)}\"\n",
    "\n",
    "sft_df = sft_train.to_pandas()\n",
    "sft_df[\"system\"] = sft_df[\"input\"].fillna(\"\")\n",
    "sft_df[\"user\"] = sft_df[\"instruction\"].fillna(\"\")\n",
    "sft_df[\"assistant\"] = sft_df[\"output\"].fillna(\"\")\n",
    "\n",
    "sft_mapped = Dataset.from_pandas(sft_df[[\"system\", \"user\", \"assistant\"]], preserve_index=False)\n",
    "\n",
    "print({\n",
    "    \"num_rows\": int(getattr(sft_mapped, \"num_rows\", 0)),\n",
    "    \"columns\": list(getattr(sft_mapped, \"features\", {}).keys()),\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "QWEN_ID = \"Qwen/Qwen2.5-0.5B\"\n",
    "qwen_tok = AutoTokenizer.from_pretrained(QWEN_ID)\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(QWEN_ID, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "\n",
    "# Ensure pad and eos are set\n",
    "if qwen_tok.pad_token is None:\n",
    "    qwen_tok.pad_token = qwen_tok.eos_token or \"<|endoftext|>\"\n",
    "\n",
    "print({\n",
    "    \"pad_token\": qwen_tok.pad_token,\n",
    "    \"pad_token_id\": qwen_tok.pad_token_id,\n",
    "    \"eos_token\": qwen_tok.eos_token,\n",
    "    \"eos_token_id\": qwen_tok.eos_token_id,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Build conversational texts via tokenizer chat template\n",
    "messages_texts = []\n",
    "for r in sft_mapped:\n",
    "    messages = []\n",
    "    sys = r[\"system\"].strip()\n",
    "    if sys:\n",
    "        messages.append({\"role\": \"system\", \"content\": sys})\n",
    "    messages.append({\"role\": \"user\", \"content\": r[\"user\"]})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": r[\"assistant\"]})\n",
    "    text = qwen_tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    messages_texts.append(text)\n",
    "\n",
    "sft_text_ds = Dataset.from_dict({\"text\": messages_texts})\n",
    "# Small validation split\n",
    "sft_splits = sft_text_ds.train_test_split(test_size=0.05, seed=SEED)\n",
    "\n",
    "sft_args = TrainingArguments(\n",
    "    output_dir=\"outputs/sft\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=32,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.0,\n",
    "    warmup_ratio=0.03,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"no\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=qwen_model,\n",
    "    tokenizer=qwen_tok,\n",
    "    args=sft_args,\n",
    "    train_dataset=sft_splits[\"train\"],\n",
    "    eval_dataset=sft_splits[\"test\"],\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "print(\"sft trainer ready\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_train_result = sft_trainer.train()\n",
    "print({k: float(v) for k, v in sft_train_result.metrics.items() if isinstance(v, (int, float))})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answers for the 4 evaluation questions\n",
    "questions_rus = [\n",
    "    \"сколько планет в нашей солнечной системе?\",\n",
    "    \"расскажи стих\",\n",
    "    \"когда собирать крыжовник?\",\n",
    "    \"Как быстро выучить новый язык?\",\n",
    "]\n",
    "\n",
    "responses = []\n",
    "for q in questions_rus:\n",
    "    msgs = []\n",
    "    if \"\" != \"\":\n",
    "        msgs.append({\"role\": \"system\", \"content\": \"\"})\n",
    "    msgs.append({\"role\": \"user\", \"content\": q})\n",
    "    prompt = qwen_tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = qwen_tok([prompt], return_tensors=\"pt\").to(qwen_model.device)\n",
    "    with torch.inference_mode():\n",
    "        out = qwen_model.generate(**inputs, max_new_tokens=128, do_sample=False, pad_token_id=qwen_tok.pad_token_id, eos_token_id=qwen_tok.eos_token_id)\n",
    "    text = qwen_tok.decode(out[0], skip_special_tokens=True)\n",
    "    responses.append({\"question\": q, \"answer\": text})\n",
    "\n",
    "for r in responses:\n",
    "    print(\"Q:\", r[\"question\"]) \n",
    "    print(\"A:\", r[\"answer\"]) \n",
    "    print(\"---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SFT model and tokenizer\n",
    "from pathlib import Path\n",
    "SFT_DIR = Path(\"outputs/sft/final\")\n",
    "SFT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sft_trainer.save_model(str(SFT_DIR))\n",
    "qwen_tok.save_pretrained(str(SFT_DIR))\n",
    "\n",
    "print({\"sft_saved_dir\": str(SFT_DIR), \"exists\": SFT_DIR.exists()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "summary = {\n",
    "    \"seed\": SEED,\n",
    "    \"device\": str(device),\n",
    "    \"device_name\": device_name,\n",
    "    \"dtype\": str(dtype),\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"torch\": torch.__version__,\n",
    "    \"transformers\": __import__(\"transformers\").__version__,\n",
    "    \"datasets\": __import__(\"datasets\").__version__,\n",
    "    \"trl\": __import__(\"trl\").__version__,\n",
    "    \"pretrain\": {\n",
    "        \"vocab_size\": int(VOCAB_SIZE),\n",
    "        \"num_params\": int(sum(p.numel() for p in model.parameters())),\n",
    "        \"context_len\": int(model.config.max_position_embeddings),\n",
    "    },\n",
    "    \"sft\": {\n",
    "        \"base_model\": QWEN_ID,\n",
    "        \"train_rows\": int(getattr(sft_splits[\"train\"], \"num_rows\", 0)),\n",
    "        \"eval_rows\": int(getattr(sft_splits[\"test\"], \"num_rows\", 0)),\n",
    "    },\n",
    "}\n",
    "print(json.dumps(summary, ensure_ascii=False, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated displays: 10 pretrain generations (last epoch) and 4 SFT answers\n",
    "import json\n",
    "\n",
    "if epoch_generations:\n",
    "    last = epoch_generations[-1]\n",
    "    print(f\"Pretrain epoch {last['epoch']} generations:\")\n",
    "    for i, g in enumerate(last[\"generations\"], 1):\n",
    "        print(f\"[{i:02d}] prompt: {g['prompt']}\")\n",
    "        print(f\"     text: {g['text']}\")\n",
    "        print(\"---\")\n",
    "else:\n",
    "    print(\"No pretrain generations recorded.\")\n",
    "\n",
    "print(\"SFT evaluation answers:\")\n",
    "for r in responses:\n",
    "    print(\"Q:\", r[\"question\"]) \n",
    "    print(\"A:\", r[\"answer\"]) \n",
    "    print(\"---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist outputs to JSON/CSV\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path(\"outputs/reports\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Pretrain generations per epoch to JSON\n",
    "with (OUT_DIR / \"pretrain_epoch_generations.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(epoch_generations, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Last epoch generations to CSV\n",
    "with (OUT_DIR / \"pretrain_last_epoch_generations.csv\").open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"epoch\", \"idx\", \"prompt\", \"text\"])\n",
    "    if epoch_generations:\n",
    "        last = epoch_generations[-1]\n",
    "        for i, g in enumerate(last[\"generations\"], 1):\n",
    "            w.writerow([last[\"epoch\"], i, g[\"prompt\"], g[\"text\"]])\n",
    "\n",
    "# SFT responses to CSV\n",
    "with (OUT_DIR / \"sft_eval_responses.csv\").open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"question\", \"answer\"])\n",
    "    for r in responses:\n",
    "        w.writerow([r[\"question\"], r[\"answer\"]])\n",
    "\n",
    "print({\"written\": True, \"dir\": str(OUT_DIR)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun Instructions\n",
    "\n",
    "- Environment setup (first time):\n",
    "  - `uv sync`\n",
    "- Launch notebook:\n",
    "  - `uv run jupyter lab`  (or `uv run jupyter notebook`)\n",
    "- Execute all cells in order. Pretrain artifacts will appear under `outputs/pretrain/`, SFT artifacts under `outputs/sft/`, and reports under `outputs/reports/`.\n",
    "- Lint (optional):\n",
    "  - `uv run ruff check --fix --unsafe-fixes .`\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
